{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Final Project: Generating Product Images from Customer Reviews\n## CMU 94-844 Generative AI Lab (Fall 2025)\n\n**Project Overview:**\n1. **Q1 (5%)**: Collect product reviews by manually copying from websites â†’ Save as `.txt` files\n2. **Q2 (7.5%)**: Analyze reviews with LLMs to extract visual and sentiment information\n3. **Q3 (7.5%)**: Generate product images using diffusion models\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "# ðŸ“‹ Quick Start Guide\n\n## How to Use This Notebook:\n\n### For Each Product:\n1. **STEP 1:** Go to product website â†’ Copy ALL review text â†’ Paste into a `.txt` file\n2. **STEP 2-4:** Run cells to define functions (only need to run once)\n3. **STEP 5+:** Run the processing cell to convert `.txt` â†’ `.csv`\n\n### Workflow:\n```\nWebsite Reviews â†’ Copy/Paste â†’ .txt file â†’ Python Parser â†’ Clean CSV files\n```\n\n### Files You'll Create:\n- `amazon_jeans_dump_data.txt` (raw text from website)\n- `walmart_chicken_dump_data.txt` (raw text from website)\n- `data/baggy_jeans_reviews.csv` (clean structured data)\n- `data/chicken_drumsticks_reviews.csv` (clean structured data)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup - Run This First"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport re\nfrom datetime import datetime\nimport os\nimport json\n\n# Create directory structure\nos.makedirs('user_input', exist_ok=True)\nos.makedirs('parsed_data', exist_ok=True)\nprint(\"âœ… Ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Q1: Product Selection and Customer Review Data Collection (5%)\n\n## Selected Products\n\n**Product 1: Baggy Jeans for Woman Men High Waisted Barrel Leg Jeans**\n- **Category:** Clothing/Fashion\n- **Link:** https://a.co/d/1Rj5tqt\n- **Rationale:** Fashion items have rich visual descriptions in reviews about fit, style, color, and texture - perfect for testing AI's ability to extract visual cues from text.\n\n**Product 2: Perdue Fresh Chicken Drumsticks**\n- **Category:** Food/Grocery  \n- **Link:** https://www.walmart.com/ip/Perdue-No-Antibiotics-Ever-Fresh-Chicken-Drumsticks-Value-Pack-4-5-5-5-lb-Tray/158751412\n- **Rationale:** Food products present unique challenges as reviews focus on quality and freshness rather than direct visual appearance.\n\n**Product 3: [TO BE ADDED]**\n- **Category:** [Electronics/Home/Beauty/etc.]\n- **Link:** [URL]\n- **Rationale:** [To be filled]\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 1: Manually Copy Review Text from Website\n\n### Instructions for Product 1 (Baggy Jeans - Amazon):\n\n1. **Go to the product page:** https://a.co/d/1Rj5tqt\n2. **Scroll to the reviews section**\n3. **Copy ALL the text** from the top of the page down through multiple reviews\n   - Include: Product title, rating, fabric info, etc.\n   - Include: All review text (reviewer names, ratings, dates, review content)\n4. **Create a new text file:** `amazon_jeans_dump_data.txt` in this folder\n5. **Paste the copied text** into the file\n6. **Add a separator line:** Type `===REVIEWS===` on a new line between the product info and the first review\n7. **Save the file**\n\n**Example format:**\n```\nBaggy Jeans for Woman Men High Waisted Barrel Leg Jeans...\n4.5 out of 5 stars    91 ratings\nFabric type82%Cotton;18%other\n...\n\n===REVIEWS===\n\nJenni\n5.0 out of 5 stars 90's Called â€” And My Teen Answered\nReviewed in the United States on November 5, 2025\nColor: Black1Size: LargeVerified Purchase\nOur 17-year-old is fully into the oversized jeans trend...\n```\n\nâœ… **Once you have created `amazon_jeans_dump_data.txt`, move to STEP 2 below**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n## STEP 2: Define Parsing Function\n\nThis function will read your `.txt` file and extract structured data from the messy text.\n\n**Run this cell to define the parser function:**"
  },
  {
   "cell_type": "code",
   "source": "def parse_amazon_reviews(file_path):\n    \"\"\"\n    Parse Amazon reviews from copied text file\n    \n    Expected format:\n    - Product info at top\n    - ===REVIEWS=== separator\n    - Reviews with pattern: Reviewer name, rating, date, review text\n    \"\"\"\n    print(f\"Reading file: {file_path}\")\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    # Extract product info (before ===REVIEWS===)\n    product_info = {}\n    \n    if '===REVIEWS===' in content:\n        header_part, reviews_part = content.split('===REVIEWS===', 1)\n        \n        # Get product title (first line)\n        lines = header_part.strip().split('\\n')\n        product_info['title'] = lines[0].strip() if lines else ''\n        \n        # Extract rating if present\n        rating_match = re.search(r'(\\d+\\.\\d+)\\s+out of 5 stars', header_part)\n        if rating_match:\n            product_info['avg_rating'] = rating_match.group(1)\n        \n        # Extract fabric/material info\n        if 'Fabric type' in header_part:\n            fabric_match = re.search(r'Fabric type(.+)', header_part)\n            if fabric_match:\n                product_info['fabric'] = fabric_match.group(1).strip()\n    else:\n        reviews_part = content\n    \n    # Parse reviews\n    reviews = []\n    \n    # Split by \"Reviewed in\" to get individual reviews\n    review_blocks = re.split(r'Reviewed in', reviews_part)\n    \n    for block in review_blocks[1:]:  # Skip first empty split\n        review = {}\n        \n        # Extract rating (X.0 out of 5 stars)\n        rating_match = re.search(r'(\\d+\\.\\d+)\\s+out of 5 stars', block)\n        if rating_match:\n            review['rating'] = rating_match.group(1)\n        else:\n            review['rating'] = ''\n        \n        # Extract date (everything after \"Reviewed in\" until next line)\n        date_match = re.search(r'^\\s*(.+?)on\\s+([^\\n]+)', block)\n        if date_match:\n            review['location'] = date_match.group(1).strip()\n            review['date'] = date_match.group(2).strip()\n        else:\n            review['location'] = ''\n            review['date'] = ''\n        \n        # Extract review title (text after stars, before \"Reviewed in\")\n        # Look backwards from \"Reviewed in\" to find the title\n        lines = block.split('\\n')\n        title = ''\n        for i, line in enumerate(lines):\n            if 'out of 5 stars' in line and i + 1 < len(lines):\n                # Title is usually right after the rating line\n                title_candidate = line.split('out of 5 stars', 1)[1].strip()\n                if title_candidate:\n                    title = title_candidate\n                    break\n        review['title'] = title\n        \n        # Extract color and size if present (improved regex to avoid capturing adjacent fields)\n        color_match = re.search(r'Color:\\s*([^S\\n]+?)(?:Size:|Verified|\\n)', block)\n        review['color'] = color_match.group(1).strip() if color_match else ''\n        \n        size_match = re.search(r'Size:\\s*([^V\\n]+?)(?:Verified|\\n)', block)\n        review['size'] = size_match.group(1).strip() if size_match else ''\n        \n        # Extract verified purchase\n        review['verified'] = 'Verified Purchase' in block\n        \n        # Extract review text (after Verified Purchase or size, before Helpful/Report)\n        # Remove metadata lines\n        text_block = block\n        # Remove everything before the first newline after color/size/verified\n        text_block = re.sub(r'^.*?Verified Purchase\\s*', '', text_block, flags=re.DOTALL)\n        # Remove Helpful/Report at the end and other footer text\n        text_block = re.sub(r'\\s*Helpful\\s*Report.*$', '', text_block, flags=re.DOTALL)\n        text_block = re.sub(r'\\s*Report\\s*$', '', text_block, flags=re.DOTALL)\n        text_block = re.sub(r'Customer image.*', '', text_block, flags=re.DOTALL)\n        text_block = re.sub(r'One person found this helpful.*', '', text_block, flags=re.DOTALL)\n        text_block = re.sub(r'\\d+ people found this helpful.*', '', text_block, flags=re.DOTALL)\n        text_block = re.sub(r'Translate review to English.*', '', text_block, flags=re.DOTALL)\n        # Remove duplicate reviewer names and ratings that appear in the text\n        text_block = re.sub(r'\\n[A-Z][a-z]+\\s+[A-Z].*?\\n\\d+\\.\\d+\\s+out of 5 stars.*?\\n', '\\n', text_block, flags=re.DOTALL)\n        \n        review['text'] = text_block.strip()\n        \n        # Only add if we have actual review text\n        if review['text'] and len(review['text']) > 10:\n            reviews.append(review)\n    \n    print(f\"âœ… Parsed {len(reviews)} reviews\")\n    print(f\"âœ… Product: {product_info.get('title', 'N/A')[:60]}...\")\n    \n    return product_info, reviews\n\nprint(\"âœ… Parser function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 3: Define Product Metadata\n\n**Run this cell to define product information:**"
  },
  {
   "cell_type": "code",
   "source": "def parse_walmart_reviews(file_path):\n    \"\"\"\n    Parse Walmart reviews from copied text file\n    \n    Format: Handles both single-line and multi-line reviews\n    - Date Name - Rating out of 5 stars - Title/Text (may continue on next lines)\n    \"\"\"\n    print(f\"Reading file: {file_path}\")\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    # Extract product info\n    product_info = {}\n    \n    if '===DESCRIPTION===' in content and '===REVIEWS===' in content:\n        desc_part = content.split('===DESCRIPTION===')[1].split('===REVIEWS===')[0]\n        \n        if 'PERDUE' in desc_part or 'Perdue' in desc_part:\n            product_info['title'] = 'PERDUE Fresh Chicken Drumsticks'\n        \n        features = []\n        if 'No Antibiotics Ever' in desc_part:\n            features.append('No Antibiotics Ever')\n        if 'protein' in desc_part:\n            protein_match = re.search(r'(\\d+g protein)', desc_part)\n            if protein_match:\n                features.append(protein_match.group(1))\n        \n        product_info['features'] = ' | '.join(features) if features else ''\n        reviews_part = content.split('===REVIEWS===')[1]\n    else:\n        reviews_part = content\n    \n    reviews = []\n    seen = set()\n    \n    lines = reviews_part.split('\\n')\n    i = 0\n    \n    while i < len(lines):\n        line = lines[i].strip()\n        \n        # Skip empty lines, page markers\n        if not line or line.startswith('Page ') or line.startswith('Walmart Chicken') or line.startswith('ig other'):\n            i += 1\n            continue\n        \n        # Match: Date Name - Rating out of 5 stars - RestOfText\n        match = re.match(r'^([A-Z][a-z]{2}\\s+\\d{1,2},\\s+\\d{4})\\s+(.+?)\\s+-\\s+(\\d+)\\s+out of 5 stars\\s+-\\s+(.*)$', line)\n        \n        if match:\n            date = match.group(1).strip()\n            reviewer = match.group(2).strip()\n            rating = match.group(3).strip()\n            first_part = match.group(4).strip()\n            \n            # Check if review continues on next lines\n            full_text = first_part\n            j = i + 1\n            while j < len(lines):\n                next_line = lines[j].strip()\n                # Stop if next review header, page marker, or empty\n                if not next_line or re.match(r'^[A-Z][a-z]{2}\\s+\\d{1,2},\\s+\\d{4}', next_line) or next_line.startswith('Page '):\n                    break\n                full_text += ' ' + next_line\n                j += 1\n            \n            i = j  # Move to next review\n            \n            # Split title and text\n            if len(first_part) < 80:\n                title = first_part\n                text = full_text\n            else:\n                parts = full_text.split('. ', 1)\n                if len(parts) == 2 and len(parts[0]) < 100:\n                    title = parts[0]\n                    text = full_text\n                else:\n                    title = full_text[:60]\n                    text = full_text\n            \n            review = {\n                'date': date,\n                'reviewer': reviewer,\n                'rating': rating,\n                'title': title,\n                'text': text,\n                'location': 'United States',\n                'verified': True\n            }\n            \n            # Avoid duplicates\n            unique_key = f\"{date}_{reviewer}_{rating}_{text[:30]}\"\n            \n            if unique_key not in seen and len(text) > 10:\n                seen.add(unique_key)\n                reviews.append(review)\n        else:\n            i += 1\n    \n    print(f\"âœ… Parsed {len(reviews)} unique reviews\")\n    print(f\"âœ… Product: {product_info.get('title', 'Walmart Product')}\")\n    \n    return product_info, reviews\n\nprint(\"âœ… Walmart parser function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## STEP 2B: Define Walmart Parser Function\n\nWalmart reviews have a different format than Amazon, so we need a separate parser.\n\n**Run this cell to define the Walmart parser:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product 1: Baggy Jeans\n",
    "product_1_info = {\n",
    "    'product_name': 'baggy_jeans',\n",
    "    'url': 'https://a.co/d/1Rj5tqt',\n",
    "    'category': 'Clothing',\n",
    "    'description': 'High waisted baggy jeans with barrel leg cut, streetwear style. 82% Cotton, 18% other materials.',\n",
    "    'features': 'High waist | Barrel leg | Loose fit | Streetwear style | Cotton blend'\n",
    "}\n",
    "\n",
    "# Product 2: Chicken Drumsticks\n",
    "product_2_info = {\n",
    "    'product_name': 'chicken_drumsticks',\n",
    "    'url': 'https://www.walmart.com/ip/158751412',\n",
    "    'category': 'Food',\n",
    "    'description': 'Fresh chicken drumsticks, no antibiotics ever, 4.5-5.5 lb value pack',\n",
    "    'features': 'No antibiotics | Fresh | Value pack | 4.5-5.5 lb'\n",
    "}\n",
    "\n",
    "# Product 3: TBD\n",
    "product_3_info = {\n",
    "    'product_name': 'product_3',\n",
    "    'url': '',\n",
    "    'category': '',\n",
    "    'description': '',\n",
    "    'features': ''\n",
    "}\n",
    "\n",
    "print(\"âœ… Product info defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 4: Define Save Function\n\nThis function saves the parsed data to CSV files.\n\n**Run this cell to define the save function:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def save_to_csv(product_info, parsed_product_info, reviews_list):\n    \"\"\"\n    Save product info and reviews to CSV\n    \n    Parameters:\n    -----------\n    product_info : dict\n        Manually entered product metadata\n    parsed_product_info : dict\n        Product info parsed from text file\n    reviews_list : list of dict\n        Parsed reviews\n    \"\"\"\n    product_name = product_info['product_name']\n    \n    # Merge product info\n    full_product_info = {**product_info, **parsed_product_info}\n    full_product_info['num_reviews'] = len(reviews_list)\n    full_product_info['collection_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    full_product_info['collection_method'] = 'manual_copy_paste'\n    \n    # Save product info to parsed_data folder\n    info_df = pd.DataFrame([full_product_info])\n    info_file = f'parsed_data/{product_name}_info.csv'\n    info_df.to_csv(info_file, index=False)\n    print(f\"\\nâœ… Saved product info: {info_file}\")\n    \n    # Save reviews to parsed_data folder\n    if reviews_list:\n        reviews_df = pd.DataFrame(reviews_list)\n        reviews_df['product_name'] = product_name\n        reviews_file = f'parsed_data/{product_name}_reviews.csv'\n        reviews_df.to_csv(reviews_file, index=False)\n        print(f\"âœ… Saved reviews: {reviews_file}\")\n        print(f\"ðŸ“Š Total reviews: {len(reviews_list)}\")\n        \n        # Show preview\n        print(f\"\\nðŸ“„ Review preview:\")\n        print(reviews_df[['rating', 'title', 'date']].head(3).to_string(index=False))\n    else:\n        print(\"âš ï¸  No reviews to save\")\n\nprint(\"âœ… Save function ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 5: Process Product 1 (Baggy Jeans)\n\n### âš ï¸ Before running this cell, make sure:\n1. âœ… You created `amazon_jeans_dump_data.txt` with the raw review text\n2. âœ… You ran all cells above (Setup, STEP 2, STEP 3, STEP 4)\n\n### What this cell does:\n1. Reads `amazon_jeans_dump_data.txt`\n2. Parses the messy text into structured data\n3. Saves to CSV files in `data/` folder:\n   - `data/baggy_jeans_info.csv` (product metadata)\n   - `data/baggy_jeans_reviews.csv` (all reviews)\n\n**Run this cell now:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parse the Amazon jeans reviews\nparsed_info, reviews = parse_amazon_reviews('user_input/amazon_jeans_dump_data.txt')\n\n# Save to CSV\nsave_to_csv(product_1_info, parsed_info, reviews)\n\nprint(\"\\nâœ… Product 1 (Baggy Jeans) complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 6: Process Product 2 (Chicken Drumsticks)\n\n### âš ï¸ Before running this cell, make sure:\n1. âœ… You created `walmart_chicken_dump_data.txt` with the raw review text\n2. âœ… You ran all cells above (Setup, STEP 2, STEP 2B, STEP 3, STEP 4)\n\n### What this cell does:\n1. Reads `walmart_chicken_dump_data.txt`\n2. Parses the Walmart format into structured data\n3. Saves to CSV files in `data/` folder:\n   - `data/chicken_drumsticks_info.csv` (product metadata)\n   - `data/chicken_drumsticks_reviews.csv` (all reviews)\n\n**Run this cell now:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parse the Walmart chicken reviews using the Walmart parser\nparsed_info_2, reviews_2 = parse_walmart_reviews('user_input/walmart_chicken_dump_data.txt')\n\n# Save to CSV\nsave_to_csv(product_2_info, parsed_info_2, reviews_2)\n\nprint(\"\\nâœ… Product 2 (Chicken Drumsticks) complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 7: Prepare Product 3\n\n**TODO:** \n1. Choose a third product from a different category (Electronics/Home/Beauty/etc.)\n2. Copy review text to a `.txt` file\n3. Update `product_3_info` dictionary above\n4. Add code here to process it"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add product 3\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 8: Verify All Collected Data\n\n**Run this cell to see a summary of all CSV files created:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import glob\n\nprint(\"ðŸ“ Collected Data Files:\")\nprint(\"=\"*70)\n\ncsv_files = sorted(glob.glob('parsed_data/*.csv'))\nif not csv_files:\n    print(\"\\nâš ï¸  No CSV files found. Run Step 5 first.\")\nelse:\n    for file in csv_files:\n        print(f\"\\nðŸ“„ {file}\")\n        df = pd.read_csv(file)\n        print(f\"   Rows: {len(df)}\")\n        print(f\"   Columns: {list(df.columns)}\")\n        print(f\"\\n   Preview:\")\n        print(df.head(2).to_string(index=False))\n        print(\"   ...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Q2: Analysis of Customer Reviews with LLM (7.5%)\n\n## Overview\nWe'll use RAG (Retrieval-Augmented Generation) with semantic chunking to analyze reviews and extract:\n1. **Visual Information** - Colors, textures, appearance details\n2. **Sentiment Analysis** - Overall sentiment and key themes\n3. **Product Features** - Main features mentioned in reviews\n4. **Image Generation Prompts** - Detailed prompts for diffusion models\n\n## Output Structure:\n- `rag_vector_store/` - FAISS indices and embeddings\n- `review_analysis_output/` - Analysis results organized by product\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n## Q2 STEP 1: Setup for LLM Analysis\n\nInstall required libraries and load API keys."
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*80)\nprint(\"ðŸ“Š Q2 ANALYSIS SUMMARY\")\nprint(\"=\"*80)\n\nprint(\"\\nðŸ“ Output Directory Structure:\")\nprint(\"  rag_vector_store/\")\nprint(\"    â”œâ”€â”€ baggy_jeans/\")\nprint(\"    â”‚   â”œâ”€â”€ faiss_index.bin\")\nprint(\"    â”‚   â”œâ”€â”€ embeddings.npy\")\nprint(\"    â”‚   â””â”€â”€ sentences.csv\")\nprint(\"    â””â”€â”€ chicken_drumsticks/\")\nprint(\"        â”œâ”€â”€ faiss_index.bin\")\nprint(\"        â”œâ”€â”€ embeddings.npy\")\nprint(\"        â””â”€â”€ sentences.csv\")\nprint(\"\")\nprint(\"  review_analysis_output/\")\nprint(\"    â”œâ”€â”€ baggy_jeans/\")\nprint(\"    â”‚   â”œâ”€â”€ visual_information.json\")\nprint(\"    â”‚   â”œâ”€â”€ product_features.json\")\nprint(\"    â”‚   â”œâ”€â”€ image_generation_prompts.json\")\nprint(\"    â”‚   â””â”€â”€ image_generation_prompts.txt\")\nprint(\"    â””â”€â”€ chicken_drumsticks/\")\nprint(\"        â”œâ”€â”€ visual_information.json\")\nprint(\"        â”œâ”€â”€ product_features.json\")\nprint(\"        â”œâ”€â”€ image_generation_prompts.json\")\nprint(\"        â””â”€â”€ image_generation_prompts.txt\")\n\n# Verify files exist\nimport os\nimport glob\n\nprint(\"\\nâœ… Verification:\")\nvector_stores = glob.glob('rag_vector_store/*/')\nprint(f\"  Vector stores created: {len(vector_stores)}\")\n\nanalysis_outputs = glob.glob('review_analysis_output/*/')\nprint(f\"  Analysis outputs created: {len(analysis_outputs)}\")\n\njson_files = glob.glob('review_analysis_output/*/*.json')\nprint(f\"  JSON analysis files: {len(json_files)}\")\n\nprint(\"\\nðŸ“Š Statistics:\")\nprint(f\"  Baggy Jeans: {len(sentences_jeans)} sentence chunks\")\nprint(f\"  Chicken Drumsticks: {len(sentences_chicken)} sentence chunks\")\nprint(f\"  Total: {len(sentences_jeans) + len(sentences_chicken)} chunks indexed\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… Q2 COMPLETE - Review Analysis with RAG\")\nprint(\"=\"*80)\nprint(\"\\nReady for Q3: Image Generation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 9: Summary and Verification\n\nVerify all outputs and generate summary.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def generate_image_prompts(product_name, visual_info, features_info, product_category):\n    \"\"\"\n    Generate image generation prompts based on analysis.\n    \"\"\"\n    print(f\"\\n### Generating Image Prompts for {product_name} ###\")\n    \n    # Combine visual and feature information\n    context = f\"\"\"\nProduct Category: {product_category}\n\nVisual Information Summary:\n{json.dumps({k: v['analysis'] for k, v in visual_info.items()}, indent=2)}\n\nFeature Information Summary:\n{json.dumps({k: v['analysis'] for k, v in features_info.items()}, indent=2)}\n\"\"\"\n    \n    # Generate prompts using LLM\n    prompt_request = \"\"\"\nBased on the customer review analysis above, generate 3 detailed image generation prompts for DALL-E or Stable Diffusion.\n\nEach prompt should:\n1. Capture the visual essence of the product from customer descriptions\n2. Include specific details about colors, textures, materials, and style\n3. Be optimized for text-to-image generation\n4. Be 2-3 sentences long\n\nFormat your response as:\nPROMPT 1: [detailed prompt]\nPROMPT 2: [detailed prompt]\nPROMPT 3: [detailed prompt]\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert at creating detailed prompts for AI image generation models.\"},\n        {\"role\": \"user\", \"content\": context + \"\\n\\n\" + prompt_request}\n    ]\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    generated_prompts = response.choices[0].message.content\n    \n    # Parse prompts\n    prompts_list = []\n    for line in generated_prompts.split('\\n'):\n        if line.startswith('PROMPT'):\n            prompt_text = line.split(':', 1)[1].strip() if ':' in line else line\n            prompts_list.append(prompt_text)\n    \n    # Save results\n    output_dir = f'review_analysis_output/{product_name}'\n    os.makedirs(output_dir, exist_ok=True)\n    \n    prompt_data = {\n        'product_name': product_name,\n        'category': product_category,\n        'generated_prompts': prompts_list,\n        'full_response': generated_prompts\n    }\n    \n    with open(f'{output_dir}/image_generation_prompts.json', 'w') as f:\n        json.dump(prompt_data, f, indent=2)\n    \n    # Also save as text file for easy reading\n    with open(f'{output_dir}/image_generation_prompts.txt', 'w') as f:\n        f.write(generated_prompts)\n    \n    print(f\"\\nâœ… Generated {len(prompts_list)} image prompts\")\n    print(f\"âœ… Saved to {output_dir}/image_generation_prompts.json\")\n    \n    print(\"\\nðŸ“ Generated Prompts:\")\n    for i, prompt in enumerate(prompts_list, 1):\n        print(f\"\\n{i}. {prompt}\")\n    \n    return prompt_data\n\nprint(\"âœ… Prompt generation function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 8B: Execute Image Prompt Generation\n\nGenerate prompts for both products.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate image prompts for both products\nprompts_jeans = generate_image_prompts(\n    'baggy_jeans', \n    visual_jeans, \n    features_jeans,\n    'Clothing - Baggy Jeans'\n)\n\nprompts_chicken = generate_image_prompts(\n    'chicken_drumsticks',\n    visual_chicken,\n    features_chicken,\n    'Food - Fresh Chicken'\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def extract_product_features(product_name, index, sentences_df, embedding_model):\n    \"\"\"\n    Extract key product features using RAG.\n    \"\"\"\n    print(f\"\\n### Extracting Product Features for {product_name} ###\")\n    \n    queries = [\n        \"What are the main features and characteristics of this product?\",\n        \"What do customers like most about this product?\",\n        \"What complaints or issues do customers mention?\",\n        \"What makes this product unique or special?\"\n    ]\n    \n    features = {}\n    \n    for query in queries:\n        print(f\"\\nQuery: {query}\")\n        \n        # Retrieve relevant sentences\n        relevant = rag_retrieve(query, index, sentences_df, embedding_model, k=15)\n        \n        # Analyze with LLM\n        analysis = rag_llm_analysis(\n            query=query,\n            context_sentences=relevant,\n            task_description=\"identify and summarize key product features from customer reviews\"\n        )\n        \n        features[query] = {\n            'analysis': analysis,\n            'top_sentences': relevant.head(5)[['sentence', 'rating', 'relevance_score']].to_dict('records')\n        }\n        \n        print(f\"Analysis: {analysis[:200]}...\")\n    \n    # Save results\n    output_dir = f'review_analysis_output/{product_name}'\n    os.makedirs(output_dir, exist_ok=True)\n    \n    with open(f'{output_dir}/product_features.json', 'w') as f:\n        json.dump(features, f, indent=2)\n    \n    print(f\"\\nâœ… Saved product features to {output_dir}/product_features.json\")\n    \n    return features\n\nprint(\"âœ… Feature extraction function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 7B: Execute Product Features Extraction\n\nRun analysis on both products.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract product features for both products\nfeatures_jeans = extract_product_features('baggy_jeans', index_jeans, sentences_jeans, model_jeans)\nfeatures_chicken = extract_product_features('chicken_drumsticks', index_chicken, sentences_chicken, model_chicken)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def extract_visual_information(product_name, index, sentences_df, embedding_model):\n    \"\"\"\n    Extract visual information from reviews using RAG.\n    \"\"\"\n    print(f\"\\n### Extracting Visual Information for {product_name} ###\")\n    \n    queries = [\n        \"What colors are mentioned in the reviews?\",\n        \"What textures and materials are described?\",\n        \"How does the product look and appear visually?\",\n        \"What visual details do customers mention?\"\n    ]\n    \n    visual_info = {}\n    \n    for query in queries:\n        print(f\"\\nQuery: {query}\")\n        \n        # Retrieve relevant sentences\n        relevant = rag_retrieve(query, index, sentences_df, embedding_model, k=15)\n        \n        # Analyze with LLM\n        analysis = rag_llm_analysis(\n            query=query,\n            context_sentences=relevant,\n            task_description=\"extract and summarize visual information from customer reviews\"\n        )\n        \n        visual_info[query] = {\n            'analysis': analysis,\n            'top_sentences': relevant.head(5)[['sentence', 'rating', 'relevance_score']].to_dict('records')\n        }\n        \n        print(f\"Analysis: {analysis[:200]}...\")\n    \n    # Save results\n    output_dir = f'review_analysis_output/{product_name}'\n    os.makedirs(output_dir, exist_ok=True)\n    \n    with open(f'{output_dir}/visual_information.json', 'w') as f:\n        json.dump(visual_info, f, indent=2)\n    \n    print(f\"\\nâœ… Saved visual information to {output_dir}/visual_information.json\")\n    \n    return visual_info\n\nprint(\"âœ… Visual extraction function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 6B: Execute Visual Information Extraction\n\nRun analysis on both products.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract visual information for both products\nvisual_jeans = extract_visual_information('baggy_jeans', index_jeans, sentences_jeans, model_jeans)\nvisual_chicken = extract_visual_information('chicken_drumsticks', index_chicken, sentences_chicken, model_chicken)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load reviews\nreviews_jeans = pd.read_csv('parsed_data/baggy_jeans_reviews.csv')\nreviews_chicken = pd.read_csv('parsed_data/chicken_drumsticks_reviews.csv')\n\nprint(f\"Loaded {len(reviews_jeans)} jeans reviews and {len(reviews_chicken)} chicken reviews\")\n\n# Process Product 1: Baggy Jeans\nprint(\"\\n### Processing Product 1: Baggy Jeans ###\")\nsentences_jeans = chunk_reviews_semantically(reviews_jeans)\nembeddings_jeans, model_jeans = create_embeddings(sentences_jeans)\nindex_jeans = build_faiss_index(embeddings_jeans)\nsave_vector_store('baggy_jeans', index_jeans, embeddings_jeans, sentences_jeans)\n\n# Process Product 2: Chicken Drumsticks\nprint(\"\\n### Processing Product 2: Chicken Drumsticks ###\")\nsentences_chicken = chunk_reviews_semantically(reviews_chicken)\nembeddings_chicken, model_chicken = create_embeddings(sentences_chicken)\nindex_chicken = build_faiss_index(embeddings_chicken)\nsave_vector_store('chicken_drumsticks', index_chicken, embeddings_chicken, sentences_chicken)\n\nprint(\"\\nâœ… Vector stores created for both products!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 5: Process Product Reviews\n\nCreate vector stores for both products.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def rag_retrieve(query, index, sentences_df, embedding_model, k=10):\n    \"\"\"\n    Retrieve top-k most relevant sentences for a query using RAG.\n    \n    Parameters:\n    -----------\n    query : str\n        The search query\n    index : faiss.Index\n        FAISS index\n    sentences_df : pd.DataFrame\n        DataFrame containing sentences\n    embedding_model : SentenceTransformer\n        Model for encoding queries\n    k : int\n        Number of results to retrieve\n    \n    Returns:\n    --------\n    pd.DataFrame : Top-k relevant sentences with metadata\n    \"\"\"\n    # Encode query\n    query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n    \n    # Search in FAISS index\n    distances, indices = index.search(query_embedding, k)\n    \n    # Get relevant sentences\n    relevant_sentences = sentences_df.iloc[indices[0]].copy()\n    relevant_sentences['distance'] = distances[0]\n    relevant_sentences['relevance_score'] = 1 / (1 + distances[0])  # Convert distance to similarity\n    \n    return relevant_sentences\n\n\ndef rag_llm_analysis(query, context_sentences, task_description, model=\"gpt-4o-mini\"):\n    \"\"\"\n    Use OpenAI with RAG context to perform analysis.\n    \n    Parameters:\n    -----------\n    query : str\n        The analysis question\n    context_sentences : pd.DataFrame\n        Retrieved relevant sentences\n    task_description : str\n        Description of the task\n    model : str\n        OpenAI model to use\n    \n    Returns:\n    --------\n    str : LLM response\n    \"\"\"\n    # Format context from retrieved sentences\n    context = \"\\n\".join([\n        f\"[Rating: {row['rating']}] {row['sentence']}\" \n        for _, row in context_sentences.iterrows()\n    ])\n    \n    # Create messages\n    messages = [\n        {\n            \"role\": \"system\", \n            \"content\": f\"You are an expert product analyst. Use the provided customer review excerpts to {task_description}.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Review excerpts:\\n{context}\\n\\nQuery: {query}\"\n        }\n    ]\n    \n    # Call OpenAI\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.3\n    )\n    \n    return response.choices[0].message.content\n\nprint(\"âœ… RAG query functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 4: RAG Query Function\n\nRetrieve relevant review sentences based on queries.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_faiss_index(embeddings):\n    \"\"\"\n    Build FAISS index for semantic search.\n    Using IndexFlatL2 for exact nearest neighbor search.\n    \"\"\"\n    dimension = embeddings.shape[1]\n    \n    print(f\"Building FAISS index with dimension: {dimension}\")\n    \n    # Create index\n    index = faiss.IndexFlatL2(dimension)\n    \n    # Add embeddings to index\n    index.add(embeddings.astype('float32'))\n    \n    print(f\"âœ… FAISS index built with {index.ntotal} vectors\")\n    \n    return index\n\n\ndef save_vector_store(product_name, index, embeddings, sentences_df):\n    \"\"\"\n    Save FAISS index, embeddings, and sentences for later use.\n    \"\"\"\n    product_dir = f'rag_vector_store/{product_name}'\n    os.makedirs(product_dir, exist_ok=True)\n    \n    # Save FAISS index\n    faiss.write_index(index, f'{product_dir}/faiss_index.bin')\n    \n    # Save embeddings\n    np.save(f'{product_dir}/embeddings.npy', embeddings)\n    \n    # Save sentences DataFrame\n    sentences_df.to_csv(f'{product_dir}/sentences.csv', index=False)\n    \n    print(f\"âœ… Saved vector store to {product_dir}/\")\n\n\ndef load_vector_store(product_name):\n    \"\"\"\n    Load saved vector store.\n    \"\"\"\n    product_dir = f'rag_vector_store/{product_name}'\n    \n    index = faiss.read_index(f'{product_dir}/faiss_index.bin')\n    embeddings = np.load(f'{product_dir}/embeddings.npy')\n    sentences_df = pd.read_csv(f'{product_dir}/sentences.csv')\n    \n    print(f\"âœ… Loaded vector store from {product_dir}/\")\n    \n    return index, embeddings, sentences_df\n\nprint(\"âœ… FAISS functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 3: Build FAISS Vector Store\n\nCreate FAISS index for efficient semantic search.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def chunk_reviews_semantically(reviews_df, text_column='text'):\n    \"\"\"\n    Split reviews into sentences for semantic chunking.\n    Inspired by HW2 approach.\n    \"\"\"\n    from nltk.tokenize import sent_tokenize\n    \n    sentences_list = []\n    \n    for idx, row in reviews_df.iterrows():\n        # Get review text\n        text = row[text_column]\n        \n        # Tokenize into sentences\n        sentences = sent_tokenize(text)\n        \n        # Store each sentence with metadata\n        for sent in sentences:\n            if len(sent.strip()) > 10:  # Filter out very short sentences\n                sentences_list.append({\n                    'review_id': idx,\n                    'product': row.get('product_name', 'unknown'),\n                    'rating': row.get('rating', ''),\n                    'sentence': sent.strip()\n                })\n    \n    sentences_df = pd.DataFrame(sentences_list)\n    print(f\"âœ… Created {len(sentences_df)} sentence chunks from {len(reviews_df)} reviews\")\n    \n    return sentences_df\n\n\ndef create_embeddings(sentences_df, model_name='all-MiniLM-L6-v2'):\n    \"\"\"\n    Create embeddings for sentences using SentenceTransformers.\n    \"\"\"\n    print(f\"Loading embedding model: {model_name}...\")\n    model = SentenceTransformer(model_name)\n    \n    sentences = sentences_df['sentence'].tolist()\n    \n    print(f\"Creating embeddings for {len(sentences)} sentences...\")\n    embeddings = model.encode(sentences, show_progress_bar=True, convert_to_numpy=True)\n    \n    print(f\"âœ… Created embeddings with shape: {embeddings.shape}\")\n    \n    return embeddings, model\n\nprint(\"âœ… Chunking functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Q2 STEP 2: Semantic Chunking Functions\n\nChunk reviews into sentences and create embeddings using sentence transformers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install required packages (run once)\n# !pip install sentence-transformers faiss-cpu openai python-dotenv nltk\n\nimport numpy as np\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport nltk\nimport os\n\n# Download NLTK data\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\n\n# Load environment variables\nload_dotenv()\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Create output directories\nos.makedirs('rag_vector_store', exist_ok=True)\nos.makedirs('review_analysis_output', exist_ok=True)\n\nprint(\"âœ… Libraries loaded and directories created\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q3: Image Generation with Diffusion Models (7.5%)\n",
    "\n",
    "**To be implemented after Q2 is complete**\n",
    "\n",
    "This section will:\n",
    "- Use prompts from Q2 to generate images\n",
    "- Test with 2 different models (DALL-E, Stable Diffusion)\n",
    "- Compare generated images with actual product images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Image generation - to be implemented\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Generating Product Images from Customer Reviews\n",
    "## CMU 94-844 Generative AI Lab (Fall 2025)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“‹ Quick Start Guide\n",
    "\n",
    "## How to Use This Notebook:\n",
    "\n",
    "### For Each Product:\n",
    "1. **STEP 1:** Go to product website â†’ Copy ALL review text â†’ Paste into a `.txt` file\n",
    "2. **STEP 2-4:** Run cells to define functions (only need to run once)\n",
    "3. **STEP 5+:** Run the processing cell to convert `.txt` â†’ `.csv`\n",
    "\n",
    "### Workflow:\n",
    "```\n",
    "Website Reviews â†’ Copy/Paste â†’ .txt file â†’ Python Parser â†’ Clean CSV files\n",
    "```\n",
    "\n",
    "### Files You'll Create:\n",
    "- `amazon_jeans_dump_data.txt` (raw text from website)\n",
    "- `walmart_chicken_dump_data.txt` (raw text from website)\n",
    "- `data/baggy_jeans_reviews.csv` (clean structured data)\n",
    "- `data/chicken_drumsticks_reviews.csv` (clean structured data)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Run This First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ready!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs('user_input', exist_ok=True)\n",
    "os.makedirs('parsed_data', exist_ok=True)\n",
    "print(\"âœ… Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Q1: Product Selection and Customer Review Data Collection (5%)\n",
    "\n",
    "## Selected Products\n",
    "\n",
    "**Product 1: Baggy Jeans for Woman Men High Waisted Barrel Leg Jeans**\n",
    "- **Category:** Clothing/Fashion\n",
    "- **Link:** https://a.co/d/1Rj5tqt\n",
    "- **Rationale:** Fashion items have rich visual descriptions in reviews about fit, style, color, and texture - perfect for testing AI's ability to extract visual cues from text.\n",
    "\n",
    "**Product 2: Perdue Fresh Chicken Drumsticks**\n",
    "- **Category:** Food/Grocery  \n",
    "- **Link:** https://www.walmart.com/ip/Perdue-No-Antibiotics-Ever-Fresh-Chicken-Drumsticks-Value-Pack-4-5-5-5-lb-Tray/158751412\n",
    "- **Rationale:** Food products present unique challenges as reviews focus on quality and freshness rather than direct visual appearance.\n",
    "\n",
    "**Product 3: [TO BE ADDED]**\n",
    "- **Category:** [Electronics/Home/Beauty/etc.]\n",
    "- **Link:** [URL]\n",
    "- **Rationale:** [To be filled]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Manually Copy Review Text from Website\n",
    "\n",
    "### Instructions for Product 1 (Baggy Jeans - Amazon):\n",
    "\n",
    "1. **Go to the product page:** https://a.co/d/1Rj5tqt\n",
    "2. **Scroll to the reviews section**\n",
    "3. **Copy ALL the text** from the top of the page down through multiple reviews\n",
    "   - Include: Product title, rating, fabric info, etc.\n",
    "   - Include: All review text (reviewer names, ratings, dates, review content)\n",
    "4. **Create a new text file:** `amazon_jeans_dump_data.txt` in this folder\n",
    "5. **Paste the copied text** into the file\n",
    "6. **Add a separator line:** Type `===REVIEWS===` on a new line between the product info and the first review\n",
    "7. **Save the file**\n",
    "\n",
    "**Example format:**\n",
    "```\n",
    "Baggy Jeans for Woman Men High Waisted Barrel Leg Jeans...\n",
    "4.5 out of 5 stars    91 ratings\n",
    "Fabric type82%Cotton;18%other\n",
    "...\n",
    "\n",
    "===REVIEWS===\n",
    "\n",
    "Jenni\n",
    "5.0 out of 5 stars 90's Called â€” And My Teen Answered\n",
    "Reviewed in the United States on November 5, 2025\n",
    "Color: Black1Size: LargeVerified Purchase\n",
    "Our 17-year-old is fully into the oversized jeans trend...\n",
    "```\n",
    "\n",
    "âœ… **Once you have created `amazon_jeans_dump_data.txt`, move to STEP 2 below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: Define Parsing Function\n",
    "\n",
    "This function will read your `.txt` file and extract structured data from the messy text.\n",
    "\n",
    "**Run this cell to define the parser function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parser function defined\n"
     ]
    }
   ],
   "source": [
    "def parse_amazon_reviews(file_path):\n",
    "    \"\"\"\n",
    "    Parse Amazon reviews from copied text file\n",
    "    \n",
    "    Expected format:\n",
    "    - Product info at top\n",
    "    - ===REVIEWS=== separator\n",
    "    - Reviews with pattern: Reviewer name, rating, date, review text\n",
    "    \"\"\"\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract product info (before ===REVIEWS===)\n",
    "    product_info = {}\n",
    "    \n",
    "    if '===REVIEWS===' in content:\n",
    "        header_part, reviews_part = content.split('===REVIEWS===', 1)\n",
    "        \n",
    "        # Get product title (first line)\n",
    "        lines = header_part.strip().split('\\n')\n",
    "        product_info['title'] = lines[0].strip() if lines else ''\n",
    "        \n",
    "        # Extract rating if present\n",
    "        rating_match = re.search(r'(\\d+\\.\\d+)\\s+out of 5 stars', header_part)\n",
    "        if rating_match:\n",
    "            product_info['avg_rating'] = rating_match.group(1)\n",
    "        \n",
    "        # Extract fabric/material info\n",
    "        if 'Fabric type' in header_part:\n",
    "            fabric_match = re.search(r'Fabric type(.+)', header_part)\n",
    "            if fabric_match:\n",
    "                product_info['fabric'] = fabric_match.group(1).strip()\n",
    "    else:\n",
    "        reviews_part = content\n",
    "    \n",
    "    # Parse reviews\n",
    "    reviews = []\n",
    "    \n",
    "    # Split by \"Reviewed in\" to get individual reviews\n",
    "    review_blocks = re.split(r'Reviewed in', reviews_part)\n",
    "    \n",
    "    for block in review_blocks[1:]:  # Skip first empty split\n",
    "        review = {}\n",
    "        \n",
    "        # Extract rating (X.0 out of 5 stars)\n",
    "        rating_match = re.search(r'(\\d+\\.\\d+)\\s+out of 5 stars', block)\n",
    "        if rating_match:\n",
    "            review['rating'] = rating_match.group(1)\n",
    "        else:\n",
    "            review['rating'] = ''\n",
    "        \n",
    "        # Extract date (everything after \"Reviewed in\" until next line)\n",
    "        date_match = re.search(r'^\\s*(.+?)on\\s+([^\\n]+)', block)\n",
    "        if date_match:\n",
    "            review['location'] = date_match.group(1).strip()\n",
    "            review['date'] = date_match.group(2).strip()\n",
    "        else:\n",
    "            review['location'] = ''\n",
    "            review['date'] = ''\n",
    "        \n",
    "        # Extract review title (text after stars, before \"Reviewed in\")\n",
    "        # Look backwards from \"Reviewed in\" to find the title\n",
    "        lines = block.split('\\n')\n",
    "        title = ''\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'out of 5 stars' in line and i + 1 < len(lines):\n",
    "                # Title is usually right after the rating line\n",
    "                title_candidate = line.split('out of 5 stars', 1)[1].strip()\n",
    "                if title_candidate:\n",
    "                    title = title_candidate\n",
    "                    break\n",
    "        review['title'] = title\n",
    "        \n",
    "        # Extract color and size if present (improved regex to avoid capturing adjacent fields)\n",
    "        color_match = re.search(r'Color:\\s*([^S\\n]+?)(?:Size:|Verified|\\n)', block)\n",
    "        review['color'] = color_match.group(1).strip() if color_match else ''\n",
    "        \n",
    "        size_match = re.search(r'Size:\\s*([^V\\n]+?)(?:Verified|\\n)', block)\n",
    "        review['size'] = size_match.group(1).strip() if size_match else ''\n",
    "        \n",
    "        # Extract verified purchase\n",
    "        review['verified'] = 'Verified Purchase' in block\n",
    "        \n",
    "        # Extract review text (after Verified Purchase or size, before Helpful/Report)\n",
    "        # Remove metadata lines\n",
    "        text_block = block\n",
    "        # Remove everything before the first newline after color/size/verified\n",
    "        text_block = re.sub(r'^.*?Verified Purchase\\s*', '', text_block, flags=re.DOTALL)\n",
    "        # Remove Helpful/Report at the end and other footer text\n",
    "        text_block = re.sub(r'\\s*Helpful\\s*Report.*$', '', text_block, flags=re.DOTALL)\n",
    "        text_block = re.sub(r'\\s*Report\\s*$', '', text_block, flags=re.DOTALL)\n",
    "        text_block = re.sub(r'Customer image.*', '', text_block, flags=re.DOTALL)\n",
    "        text_block = re.sub(r'One person found this helpful.*', '', text_block, flags=re.DOTALL)\n",
    "        text_block = re.sub(r'\\d+ people found this helpful.*', '', text_block, flags=re.DOTALL)\n",
    "        text_block = re.sub(r'Translate review to English.*', '', text_block, flags=re.DOTALL)\n",
    "        # Remove duplicate reviewer names and ratings that appear in the text\n",
    "        text_block = re.sub(r'\\n[A-Z][a-z]+\\s+[A-Z].*?\\n\\d+\\.\\d+\\s+out of 5 stars.*?\\n', '\\n', text_block, flags=re.DOTALL)\n",
    "        \n",
    "        review['text'] = text_block.strip()\n",
    "        \n",
    "        # Only add if we have actual review text\n",
    "        if review['text'] and len(review['text']) > 10:\n",
    "            reviews.append(review)\n",
    "    \n",
    "    print(f\"âœ… Parsed {len(reviews)} reviews\")\n",
    "    print(f\"âœ… Product: {product_info.get('title', 'N/A')[:60]}...\")\n",
    "    \n",
    "    return product_info, reviews\n",
    "\n",
    "print(\"âœ… Parser function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: Define Product Metadata\n",
    "\n",
    "**Run this cell to define product information:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Walmart parser function defined\n"
     ]
    }
   ],
   "source": [
    "def parse_walmart_reviews(file_path):\n",
    "    \"\"\"\n",
    "    Parse Walmart reviews from copied text file\n",
    "    \n",
    "    Format: Handles both single-line and multi-line reviews\n",
    "    - Date Name - Rating out of 5 stars - Title/Text (may continue on next lines)\n",
    "    \"\"\"\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract product info\n",
    "    product_info = {}\n",
    "    \n",
    "    if '===DESCRIPTION===' in content and '===REVIEWS===' in content:\n",
    "        desc_part = content.split('===DESCRIPTION===')[1].split('===REVIEWS===')[0]\n",
    "        \n",
    "        if 'PERDUE' in desc_part or 'Perdue' in desc_part:\n",
    "            product_info['title'] = 'PERDUE Fresh Chicken Drumsticks'\n",
    "        \n",
    "        features = []\n",
    "        if 'No Antibiotics Ever' in desc_part:\n",
    "            features.append('No Antibiotics Ever')\n",
    "        if 'protein' in desc_part:\n",
    "            protein_match = re.search(r'(\\d+g protein)', desc_part)\n",
    "            if protein_match:\n",
    "                features.append(protein_match.group(1))\n",
    "        \n",
    "        product_info['features'] = ' | '.join(features) if features else ''\n",
    "        reviews_part = content.split('===REVIEWS===')[1]\n",
    "    else:\n",
    "        reviews_part = content\n",
    "    \n",
    "    reviews = []\n",
    "    seen = set()\n",
    "    \n",
    "    lines = reviews_part.split('\\n')\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Skip empty lines, page markers\n",
    "        if not line or line.startswith('Page ') or line.startswith('Walmart Chicken') or line.startswith('ig other'):\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Match: Date Name - Rating out of 5 stars - RestOfText\n",
    "        match = re.match(r'^([A-Z][a-z]{2}\\s+\\d{1,2},\\s+\\d{4})\\s+(.+?)\\s+-\\s+(\\d+)\\s+out of 5 stars\\s+-\\s+(.*)$', line)\n",
    "        \n",
    "        if match:\n",
    "            date = match.group(1).strip()\n",
    "            reviewer = match.group(2).strip()\n",
    "            rating = match.group(3).strip()\n",
    "            first_part = match.group(4).strip()\n",
    "            \n",
    "            # Check if review continues on next lines\n",
    "            full_text = first_part\n",
    "            j = i + 1\n",
    "            while j < len(lines):\n",
    "                next_line = lines[j].strip()\n",
    "                # Stop if next review header, page marker, or empty\n",
    "                if not next_line or re.match(r'^[A-Z][a-z]{2}\\s+\\d{1,2},\\s+\\d{4}', next_line) or next_line.startswith('Page '):\n",
    "                    break\n",
    "                full_text += ' ' + next_line\n",
    "                j += 1\n",
    "            \n",
    "            i = j  # Move to next review\n",
    "            \n",
    "            # Split title and text\n",
    "            if len(first_part) < 80:\n",
    "                title = first_part\n",
    "                text = full_text\n",
    "            else:\n",
    "                parts = full_text.split('. ', 1)\n",
    "                if len(parts) == 2 and len(parts[0]) < 100:\n",
    "                    title = parts[0]\n",
    "                    text = full_text\n",
    "                else:\n",
    "                    title = full_text[:60]\n",
    "                    text = full_text\n",
    "            \n",
    "            review = {\n",
    "                'date': date,\n",
    "                'reviewer': reviewer,\n",
    "                'rating': rating,\n",
    "                'title': title,\n",
    "                'text': text,\n",
    "                'location': 'United States',\n",
    "                'verified': True\n",
    "            }\n",
    "            \n",
    "            # Avoid duplicates\n",
    "            unique_key = f\"{date}_{reviewer}_{rating}_{text[:30]}\"\n",
    "            \n",
    "            if unique_key not in seen and len(text) > 10:\n",
    "                seen.add(unique_key)\n",
    "                reviews.append(review)\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    print(f\"âœ… Parsed {len(reviews)} unique reviews\")\n",
    "    print(f\"âœ… Product: {product_info.get('title', 'Walmart Product')}\")\n",
    "    \n",
    "    return product_info, reviews\n",
    "\n",
    "print(\"âœ… Walmart parser function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2B: Define Walmart Parser Function\n",
    "\n",
    "Walmart reviews have a different format than Amazon, so we need a separate parser.\n",
    "\n",
    "**Run this cell to define the Walmart parser:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Product info defined\n"
     ]
    }
   ],
   "source": [
    "# Product 1: Baggy Jeans\n",
    "product_1_info = {\n",
    "    'product_name': 'baggy_jeans',\n",
    "    'url': 'https://a.co/d/1Rj5tqt',\n",
    "    'category': 'Clothing',\n",
    "    'description': 'High waisted baggy jeans with barrel leg cut, streetwear style. 82% Cotton, 18% other materials.',\n",
    "    'features': 'High waist | Barrel leg | Loose fit | Streetwear style | Cotton blend'\n",
    "}\n",
    "\n",
    "# Product 2: Chicken Drumsticks\n",
    "product_2_info = {\n",
    "    'product_name': 'chicken_drumsticks',\n",
    "    'url': 'https://www.walmart.com/ip/158751412',\n",
    "    'category': 'Food',\n",
    "    'description': 'Fresh chicken drumsticks, no antibiotics ever, 4.5-5.5 lb value pack',\n",
    "    'features': 'No antibiotics | Fresh | Value pack | 4.5-5.5 lb'\n",
    "}\n",
    "\n",
    "# Product 3: TBD\n",
    "product_3_info = {\n",
    "    'product_name': 'product_3',\n",
    "    'url': '',\n",
    "    'category': '',\n",
    "    'description': '',\n",
    "    'features': ''\n",
    "}\n",
    "\n",
    "print(\"âœ… Product info defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4: Define Save Function\n",
    "\n",
    "This function saves the parsed data to CSV files.\n",
    "\n",
    "**Run this cell to define the save function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Save function ready\n"
     ]
    }
   ],
   "source": [
    "def save_to_csv(product_info, parsed_product_info, reviews_list):\n",
    "    \"\"\"\n",
    "    Save product info and reviews to CSV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    product_info : dict\n",
    "        Manually entered product metadata\n",
    "    parsed_product_info : dict\n",
    "        Product info parsed from text file\n",
    "    reviews_list : list of dict\n",
    "        Parsed reviews\n",
    "    \"\"\"\n",
    "    product_name = product_info['product_name']\n",
    "    \n",
    "    # Merge product info\n",
    "    full_product_info = {**product_info, **parsed_product_info}\n",
    "    full_product_info['num_reviews'] = len(reviews_list)\n",
    "    full_product_info['collection_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    full_product_info['collection_method'] = 'manual_copy_paste'\n",
    "    \n",
    "    # Save product info to parsed_data folder\n",
    "    info_df = pd.DataFrame([full_product_info])\n",
    "    info_file = f'parsed_data/{product_name}_info.csv'\n",
    "    info_df.to_csv(info_file, index=False)\n",
    "    print(f\"\\nâœ… Saved product info: {info_file}\")\n",
    "    \n",
    "    # Save reviews to parsed_data folder\n",
    "    if reviews_list:\n",
    "        reviews_df = pd.DataFrame(reviews_list)\n",
    "        reviews_df['product_name'] = product_name\n",
    "        reviews_file = f'parsed_data/{product_name}_reviews.csv'\n",
    "        reviews_df.to_csv(reviews_file, index=False)\n",
    "        print(f\"âœ… Saved reviews: {reviews_file}\")\n",
    "        print(f\"ðŸ“Š Total reviews: {len(reviews_list)}\")\n",
    "        \n",
    "        # Show preview\n",
    "        print(f\"\\nðŸ“„ Review preview:\")\n",
    "        print(reviews_df[['rating', 'title', 'date']].head(3).to_string(index=False))\n",
    "    else:\n",
    "        print(\"âš ï¸  No reviews to save\")\n",
    "\n",
    "print(\"âœ… Save function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 5: Process Product 1 (Baggy Jeans)\n",
    "\n",
    "### âš ï¸ Before running this cell, make sure:\n",
    "1. âœ… You created `amazon_jeans_dump_data.txt` with the raw review text\n",
    "2. âœ… You ran all cells above (Setup, STEP 2, STEP 3, STEP 4)\n",
    "\n",
    "### What this cell does:\n",
    "1. Reads `amazon_jeans_dump_data.txt`\n",
    "2. Parses the messy text into structured data\n",
    "3. Saves to CSV files in `data/` folder:\n",
    "   - `data/baggy_jeans_info.csv` (product metadata)\n",
    "   - `data/baggy_jeans_reviews.csv` (all reviews)\n",
    "\n",
    "**Run this cell now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: user_input/amazon_jeans_dump_data.txt\n",
      "âœ… Parsed 10 reviews\n",
      "âœ… Product: Baggy Jeans for Woman Men High Waisted Barrel Leg Jeans Casu...\n",
      "\n",
      "âœ… Saved product info: parsed_data/baggy_jeans_info.csv\n",
      "âœ… Saved reviews: parsed_data/baggy_jeans_reviews.csv\n",
      "ðŸ“Š Total reviews: 10\n",
      "\n",
      "ðŸ“„ Review preview:\n",
      "rating                                      title             date\n",
      "   5.0 Love these Jeans! Super cute and on-trend! November 5, 2025\n",
      "   5.0                               Really great     June 1, 2025\n",
      "   4.0                                 Good jeans October 26, 2025\n",
      "\n",
      "âœ… Product 1 (Baggy Jeans) complete!\n"
     ]
    }
   ],
   "source": [
    "# Parse the Amazon jeans reviews\n",
    "parsed_info, reviews = parse_amazon_reviews('user_input/amazon_jeans_dump_data.txt')\n",
    "\n",
    "# Save to CSV\n",
    "save_to_csv(product_1_info, parsed_info, reviews)\n",
    "\n",
    "print(\"\\nâœ… Product 1 (Baggy Jeans) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 6: Process Product 2 (Chicken Drumsticks)\n",
    "\n",
    "### âš ï¸ Before running this cell, make sure:\n",
    "1. âœ… You created `walmart_chicken_dump_data.txt` with the raw review text\n",
    "2. âœ… You ran all cells above (Setup, STEP 2, STEP 2B, STEP 3, STEP 4)\n",
    "\n",
    "### What this cell does:\n",
    "1. Reads `walmart_chicken_dump_data.txt`\n",
    "2. Parses the Walmart format into structured data\n",
    "3. Saves to CSV files in `data/` folder:\n",
    "   - `data/chicken_drumsticks_info.csv` (product metadata)\n",
    "   - `data/chicken_drumsticks_reviews.csv` (all reviews)\n",
    "\n",
    "**Run this cell now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: user_input/walmart_chicken_dump_data.txt\n",
      "âœ… Parsed 40 unique reviews\n",
      "âœ… Product: PERDUE Fresh Chicken Drumsticks\n",
      "\n",
      "âœ… Saved product info: parsed_data/chicken_drumsticks_info.csv\n",
      "âœ… Saved reviews: parsed_data/chicken_drumsticks_reviews.csv\n",
      "ðŸ“Š Total reviews: 40\n",
      "\n",
      "ðŸ“„ Review preview:\n",
      "rating                                                        title         date\n",
      "     5 We only buy Perdue!!! And only at Walmart! We love our Perdu Aug 18, 2025\n",
      "     5 refund request: I had an issue with the cheese slices that I Jul 19, 2025\n",
      "     5 Perdue drumstickas are a must in every fridge! These Perdue  Jul 15, 2025\n",
      "\n",
      "âœ… Product 2 (Chicken Drumsticks) complete!\n"
     ]
    }
   ],
   "source": [
    "# Parse the Walmart chicken reviews using the Walmart parser\n",
    "parsed_info_2, reviews_2 = parse_walmart_reviews('user_input/walmart_chicken_dump_data.txt')\n",
    "\n",
    "# Save to CSV\n",
    "save_to_csv(product_2_info, parsed_info_2, reviews_2)\n",
    "\n",
    "print(\"\\nâœ… Product 2 (Chicken Drumsticks) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 7: Prepare Product 3\n",
    "\n",
    "**TODO:** \n",
    "1. Choose a third product from a different category (Electronics/Home/Beauty/etc.)\n",
    "2. Copy review text to a `.txt` file\n",
    "3. Update `product_3_info` dictionary above\n",
    "4. Add code here to process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add product 3\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 8: Verify All Collected Data\n",
    "\n",
    "**Run this cell to see a summary of all CSV files created:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Collected Data Files:\n",
      "======================================================================\n",
      "\n",
      "ðŸ“„ parsed_data/baggy_jeans_info.csv\n",
      "   Rows: 1\n",
      "   Columns: ['product_name', 'url', 'category', 'description', 'features', 'title', 'avg_rating', 'fabric', 'num_reviews', 'collection_date', 'collection_method']\n",
      "\n",
      "   Preview:\n",
      "product_name                    url category                                                                                      description                                                              features                                                                                               title  avg_rating             fabric  num_reviews     collection_date collection_method\n",
      " baggy_jeans https://a.co/d/1Rj5tqt Clothing High waisted baggy jeans with barrel leg cut, streetwear style. 82% Cotton, 18% other materials. High waist | Barrel leg | Loose fit | Streetwear style | Cotton blend Baggy Jeans for Woman Men High Waisted Barrel Leg Jeans Casual Loose Denim Pants Streetwear Clothes         4.5 82%Cotton;18%other           10 2025-11-24 00:03:11 manual_copy_paste\n",
      "   ...\n",
      "\n",
      "ðŸ“„ parsed_data/baggy_jeans_reviews.csv\n",
      "   Rows: 10\n",
      "   Columns: ['rating', 'location', 'date', 'title', 'color', 'size', 'verified', 'text', 'product_name']\n",
      "\n",
      "   Preview:\n",
      " rating          location             date                                      title  color   size  verified                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text product_name\n",
      "    5.0 the United States November 5, 2025 Love these Jeans! Super cute and on-trend! Black1  Large      True                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Our 17-year-old is fully into the oversized jeans trend (wooo, 90â€™s style here we come!). He wanted these and absolutely loves them. They fit perfectly â€” well, perfectly for oversized ðŸ¤”. Great quality and totally on-trend.  baggy_jeans\n",
      "    5.0 the United States     June 1, 2025                               Really great  Blue1 Medium      True Love these Jeans! I got the size Med, Iâ€™m 5'1\" 128lbs. I normally wear a 28-30 waist, so they are just a little loose on my waist, but I hope they will tighten up and shrink in a hot wash and hot dryer. These are very hip-hop, cool-girl jeans. They are very loose fitting everywhere and are not a stretch denim. They are high-waisted. The jeans have cute metal rivets on the pockets and thank god there is no dumb logo on the pocket (or I would have sent em back). Jeans are a dark'ish navy blue and a nice med-weight denim, not stiff. Because I am short I will need to cut a lot off the bottom. Keep in mind if you do cut a lot off the bottom they may loose their barrel leg shape.\\nMeasurements for the Med. I was sent:\\nJeans Front inseam -13\"\\nJeans Back inseam -16\"\\nJeans waist -34\"\\nJeans length from top of waistband to bottom - 44\"  baggy_jeans\n",
      "   ...\n",
      "\n",
      "ðŸ“„ parsed_data/chicken_drumsticks_info.csv\n",
      "   Rows: 1\n",
      "   Columns: ['product_name', 'url', 'category', 'description', 'features', 'title', 'num_reviews', 'collection_date', 'collection_method']\n",
      "\n",
      "   Preview:\n",
      "      product_name                                  url category                                                          description                          features                           title  num_reviews     collection_date collection_method\n",
      "chicken_drumsticks https://www.walmart.com/ip/158751412     Food Fresh chicken drumsticks, no antibiotics ever, 4.5-5.5 lb value pack No Antibiotics Ever | 20g protein PERDUE Fresh Chicken Drumsticks           40 2025-11-24 00:03:11 manual_copy_paste\n",
      "   ...\n",
      "\n",
      "ðŸ“„ parsed_data/chicken_drumsticks_reviews.csv\n",
      "   Rows: 40\n",
      "   Columns: ['date', 'reviewer', 'rating', 'title', 'text', 'location', 'verified', 'product_name']\n",
      "\n",
      "   Preview:\n",
      "        date  reviewer  rating                                                        title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text      location  verified       product_name\n",
      "Aug 18, 2025 anonymous       5 We only buy Perdue!!! And only at Walmart! We love our Perdu We only buy Perdue!!! And only at Walmart! We love our Perdue! We will not buy any other brand of chicken! The flavor is the best and itâ€™s always fresh. My favorite for the chicken drumsticks is to make my own home made bone broth. So easy! Throw and entire pkg. of family size drumsticks (skin on) into a crock pot. Cover with water until about 1/2 in. above the chicken. Cook on low for 12 hrs. I usually do mine overnight. And there you go! You not only have a drinkable broth (add salt and pepper to taste after itâ€™s cooked), but you can also use it for soups and gravy. Then I use the chicken for various dishes like chicken salad, warm chicken toasted sandwich and even for the soup. You truly canâ€™t beat the price of what you get from one package as far as meals go. Walmartâ€™s prices beat all the grocery stores in our area for the Perdue Family Size Drumsticks. United States      True chicken_drumsticks\n",
      "Jul 19, 2025    joanne       5 refund request: I had an issue with the cheese slices that I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     refund request: I had an issue with the cheese slices that I bought and I sent her a request for a refund but havenâ€™t heard anything back on it yet the problem was the cheese was sticking so badly to the plastic wrappers couldnâ€™t get the cheese out of the wrapper. the refund was for $2.48 United States      True chicken_drumsticks\n",
      "   ...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "print(\"ðŸ“ Collected Data Files:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "csv_files = sorted(glob.glob('parsed_data/*.csv'))\n",
    "if not csv_files:\n",
    "    print(\"\\nâš ï¸  No CSV files found. Run Step 5 first.\")\n",
    "else:\n",
    "    for file in csv_files:\n",
    "        print(f\"\\nðŸ“„ {file}\")\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"   Rows: {len(df)}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        print(f\"\\n   Preview:\")\n",
    "        print(df.head(2).to_string(index=False))\n",
    "        print(\"   ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Q2: Analysis of Customer Reviews with LLM\n",
    "\n",
    "## Overview\n",
    "We'll use RAG (Retrieval-Augmented Generation) with semantic chunking to analyze reviews and extract:\n",
    "1. **Visual Information** - Colors, textures, appearance details\n",
    "2. **Sentiment Analysis** - Overall sentiment and key themes\n",
    "3. **Product Features** - Main features mentioned in reviews\n",
    "4. **Image Generation Prompts** - Detailed prompts for diffusion models\n",
    "\n",
    "## Output Structure:\n",
    "- `rag_vector_store/` - FAISS indices and embeddings\n",
    "- `review_analysis_output/` - Analysis results organized by product\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded and directories created\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install sentence-transformers faiss-cpu openai python-dotenv nltk\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('rag_vector_store', exist_ok=True)\n",
    "os.makedirs('review_analysis_output', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Libraries loaded and directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 2: Semantic Chunking Functions\n",
    "\n",
    "Chunk reviews into sentences and create embeddings using sentence transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunking functions defined\n"
     ]
    }
   ],
   "source": [
    "def chunk_reviews_semantically(reviews_df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Split reviews into sentences for semantic chunking.\n",
    "    Inspired by HW2 approach.\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "    sentences_list = []\n",
    "    \n",
    "    for idx, row in reviews_df.iterrows():\n",
    "        # Get review text\n",
    "        text = row[text_column]\n",
    "        \n",
    "        # Tokenize into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Store each sentence with metadata\n",
    "        for sent in sentences:\n",
    "            if len(sent.strip()) > 10:  # Filter out very short sentences\n",
    "                sentences_list.append({\n",
    "                    'review_id': idx,\n",
    "                    'product': row.get('product_name', 'unknown'),\n",
    "                    'rating': row.get('rating', ''),\n",
    "                    'sentence': sent.strip()\n",
    "                })\n",
    "    \n",
    "    sentences_df = pd.DataFrame(sentences_list)\n",
    "    print(f\"âœ… Created {len(sentences_df)} sentence chunks from {len(reviews_df)} reviews\")\n",
    "    \n",
    "    return sentences_df\n",
    "\n",
    "\n",
    "def create_embeddings(sentences_df, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Create embeddings for sentences using SentenceTransformers.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    sentences = sentences_df['sentence'].tolist()\n",
    "    \n",
    "    print(f\"Creating embeddings for {len(sentences)} sentences...\")\n",
    "    embeddings = model.encode(sentences, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    print(f\"âœ… Created embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings, model\n",
    "\n",
    "print(\"âœ… Chunking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 3: Build FAISS Vector Store\n",
    "\n",
    "Create FAISS index for efficient semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS functions defined\n"
     ]
    }
   ],
   "source": [
    "def build_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Build FAISS index for semantic search.\n",
    "    Using IndexFlatL2 for exact nearest neighbor search.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    print(f\"Building FAISS index with dimension: {dimension}\")\n",
    "    \n",
    "    # Create index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    print(f\"âœ… FAISS index built with {index.ntotal} vectors\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "def save_vector_store(product_name, index, embeddings, sentences_df):\n",
    "    \"\"\"\n",
    "    Save FAISS index, embeddings, and sentences for later use.\n",
    "    \"\"\"\n",
    "    product_dir = f'rag_vector_store/{product_name}'\n",
    "    os.makedirs(product_dir, exist_ok=True)\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, f'{product_dir}/faiss_index.bin')\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save(f'{product_dir}/embeddings.npy', embeddings)\n",
    "    \n",
    "    # Save sentences DataFrame\n",
    "    sentences_df.to_csv(f'{product_dir}/sentences.csv', index=False)\n",
    "    \n",
    "    print(f\"âœ… Saved vector store to {product_dir}/\")\n",
    "\n",
    "\n",
    "def load_vector_store(product_name):\n",
    "    \"\"\"\n",
    "    Load saved vector store.\n",
    "    \"\"\"\n",
    "    product_dir = f'rag_vector_store/{product_name}'\n",
    "    \n",
    "    index = faiss.read_index(f'{product_dir}/faiss_index.bin')\n",
    "    embeddings = np.load(f'{product_dir}/embeddings.npy')\n",
    "    sentences_df = pd.read_csv(f'{product_dir}/sentences.csv')\n",
    "    \n",
    "    print(f\"âœ… Loaded vector store from {product_dir}/\")\n",
    "    \n",
    "    return index, embeddings, sentences_df\n",
    "\n",
    "print(\"âœ… FAISS functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 4: RAG Query Function\n",
    "\n",
    "Retrieve relevant review sentences based on queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG query functions defined\n"
     ]
    }
   ],
   "source": [
    "def rag_retrieve(query, index, sentences_df, embedding_model, k=10):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant sentences for a query using RAG.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        The search query\n",
    "    index : faiss.Index\n",
    "        FAISS index\n",
    "    sentences_df : pd.DataFrame\n",
    "        DataFrame containing sentences\n",
    "    embedding_model : SentenceTransformer\n",
    "        Model for encoding queries\n",
    "    k : int\n",
    "        Number of results to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Top-k relevant sentences with metadata\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Get relevant sentences\n",
    "    relevant_sentences = sentences_df.iloc[indices[0]].copy()\n",
    "    relevant_sentences['distance'] = distances[0]\n",
    "    relevant_sentences['relevance_score'] = 1 / (1 + distances[0])  # Convert distance to similarity\n",
    "    \n",
    "    return relevant_sentences\n",
    "\n",
    "\n",
    "def rag_llm_analysis(query, context_sentences, task_description, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Use OpenAI with RAG context to perform analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        The analysis question\n",
    "    context_sentences : pd.DataFrame\n",
    "        Retrieved relevant sentences\n",
    "    task_description : str\n",
    "        Description of the task\n",
    "    model : str\n",
    "        OpenAI model to use\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : LLM response\n",
    "    \"\"\"\n",
    "    # Format context from retrieved sentences\n",
    "    context = \"\\n\".join([\n",
    "        f\"[Rating: {row['rating']}] {row['sentence']}\" \n",
    "        for _, row in context_sentences.iterrows()\n",
    "    ])\n",
    "    \n",
    "    # Create messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"You are an expert product analyst. Use the provided customer review excerpts to {task_description}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Review excerpts:\\n{context}\\n\\nQuery: {query}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Call OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"âœ… RAG query functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 5: Process Product Reviews\n",
    "\n",
    "Create vector stores for both products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 jeans reviews and 40 chicken reviews\n",
      "\n",
      "### Processing Product 1: Baggy Jeans ###\n",
      "âœ… Created 52 sentence chunks from 10 reviews\n",
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Creating embeddings for 52 sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b3ca4be6204ec2bad68fc765f76300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created embeddings with shape: (52, 384)\n",
      "Building FAISS index with dimension: 384\n",
      "âœ… FAISS index built with 52 vectors\n",
      "âœ… Saved vector store to rag_vector_store/baggy_jeans/\n",
      "\n",
      "### Processing Product 2: Chicken Drumsticks ###\n",
      "âœ… Created 103 sentence chunks from 40 reviews\n",
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Creating embeddings for 103 sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45a38d6eeb4494d89c11d02b31adbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created embeddings with shape: (103, 384)\n",
      "Building FAISS index with dimension: 384\n",
      "âœ… FAISS index built with 103 vectors\n",
      "âœ… Saved vector store to rag_vector_store/chicken_drumsticks/\n",
      "\n",
      "âœ… Vector stores created for both products!\n"
     ]
    }
   ],
   "source": [
    "# Load reviews\n",
    "reviews_jeans = pd.read_csv('parsed_data/baggy_jeans_reviews.csv')\n",
    "reviews_chicken = pd.read_csv('parsed_data/chicken_drumsticks_reviews.csv')\n",
    "\n",
    "print(f\"Loaded {len(reviews_jeans)} jeans reviews and {len(reviews_chicken)} chicken reviews\")\n",
    "\n",
    "# Process Product 1: Baggy Jeans\n",
    "print(\"\\n### Processing Product 1: Baggy Jeans ###\")\n",
    "sentences_jeans = chunk_reviews_semantically(reviews_jeans)\n",
    "embeddings_jeans, model_jeans = create_embeddings(sentences_jeans)\n",
    "index_jeans = build_faiss_index(embeddings_jeans)\n",
    "save_vector_store('baggy_jeans', index_jeans, embeddings_jeans, sentences_jeans)\n",
    "\n",
    "# Process Product 2: Chicken Drumsticks\n",
    "print(\"\\n### Processing Product 2: Chicken Drumsticks ###\")\n",
    "sentences_chicken = chunk_reviews_semantically(reviews_chicken)\n",
    "embeddings_chicken, model_chicken = create_embeddings(sentences_chicken)\n",
    "index_chicken = build_faiss_index(embeddings_chicken)\n",
    "save_vector_store('chicken_drumsticks', index_chicken, embeddings_chicken, sentences_chicken)\n",
    "\n",
    "print(\"\\nâœ… Vector stores created for both products!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 6: Analysis Module 1 - Visual Information Extraction\n",
    "\n",
    "Extract colors, textures, materials, and visual details from reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Extracting Visual Information for baggy_jeans ###\n",
      "\n",
      "Query: What colors are mentioned in the reviews?\n",
      "Analysis: The reviews mention the following colors:\n",
      "\n",
      "1. Dark'ish navy blue\n",
      "2. Yellow (specifically in reference to the embroidery and stitches)...\n",
      "\n",
      "Query: What textures and materials are described?\n",
      "Analysis: The customer reviews describe the jeans as being made from a medium-weight denim that is not stiff, indicating a comfortable texture. One review mentions that the material is \"nothing special\" and \"qu...\n",
      "\n",
      "Query: How does the product look and appear visually?\n",
      "Analysis: Based on the customer reviews, the product, which appears to be a pair of jeans, has several notable visual characteristics:\n",
      "\n",
      "1. **Color and Stitching**: The jeans feature yellow stitching, which is h...\n",
      "\n",
      "Query: What visual details do customers mention?\n",
      "Analysis: Customers mention several visual details in their reviews of the jeans:\n",
      "\n",
      "1. **Quality and Trendiness**: The jeans are described as having great quality and being on-trend, indicating a modern and styl...\n",
      "\n",
      "âœ… Saved visual information to review_analysis_output/baggy_jeans/visual_information.json\n",
      "\n",
      "### Extracting Visual Information for chicken_drumsticks ###\n",
      "\n",
      "Query: What colors are mentioned in the reviews?\n",
      "Analysis: The colors mentioned in the reviews are greyish and brown....\n",
      "\n",
      "Query: What textures and materials are described?\n",
      "Analysis: The customer reviews describe several textures and materials related to the product:\n",
      "\n",
      "1. **Color and Appearance**: \n",
      "   - Some reviews mention a \"greyish color,\" indicating a potentially undesirable vi...\n",
      "\n",
      "Query: How does the product look and appear visually?\n",
      "Analysis: Based on the customer review excerpts, the visual information regarding the product can be summarized as follows:\n",
      "\n",
      "1. **Color**: There are mixed reviews about the color of the product. One reviewer me...\n",
      "\n",
      "Query: What visual details do customers mention?\n",
      "Analysis: From the customer reviews, the following visual details are mentioned:\n",
      "\n",
      "1. **Quality of Products**: Customers highlight the appearance and quality of the chicken, with mentions of it being \"top qualit...\n",
      "\n",
      "âœ… Saved visual information to review_analysis_output/chicken_drumsticks/visual_information.json\n"
     ]
    }
   ],
   "source": [
    "def extract_visual_information(product_name, index, sentences_df, embedding_model):\n",
    "    \"\"\"\n",
    "    Extract visual information from reviews using RAG.\n",
    "    \"\"\"\n",
    "    print(f\"\\n### Extracting Visual Information for {product_name} ###\")\n",
    "    \n",
    "    queries = [\n",
    "        \"What colors are mentioned in the reviews?\",\n",
    "        \"What textures and materials are described?\",\n",
    "        \"How does the product look and appear visually?\",\n",
    "        \"What visual details do customers mention?\"\n",
    "    ]\n",
    "    \n",
    "    visual_info = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        \n",
    "        # Retrieve relevant sentences\n",
    "        relevant = rag_retrieve(query, index, sentences_df, embedding_model, k=15)\n",
    "        \n",
    "        # Analyze with LLM\n",
    "        analysis = rag_llm_analysis(\n",
    "            query=query,\n",
    "            context_sentences=relevant,\n",
    "            task_description=\"extract and summarize visual information from customer reviews\"\n",
    "        )\n",
    "        \n",
    "        visual_info[query] = {\n",
    "            'analysis': analysis,\n",
    "            'top_sentences': relevant.head(5)[['sentence', 'rating', 'relevance_score']].to_dict('records')\n",
    "        }\n",
    "        \n",
    "        print(f\"Analysis: {analysis[:200]}...\")\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = f'review_analysis_output/{product_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(f'{output_dir}/visual_information.json', 'w') as f:\n",
    "        json.dump(visual_info, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Saved visual information to {output_dir}/visual_information.json\")\n",
    "    \n",
    "    return visual_info\n",
    "\n",
    "# Extract for both products\n",
    "visual_jeans = extract_visual_information('baggy_jeans', index_jeans, sentences_jeans, model_jeans)\n",
    "visual_chicken = extract_visual_information('chicken_drumsticks', index_chicken, sentences_chicken, model_chicken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 7: Analysis Module 2 - Product Features Extraction\n",
    "\n",
    "Extract key product features, benefits, and drawbacks mentioned in reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Extracting Product Features for baggy_jeans ###\n",
      "\n",
      "Query: What are the main features and characteristics of this product?\n",
      "Analysis: Based on the customer review excerpts, the main features and characteristics of the product (likely a pair of jeans) are as follows:\n",
      "\n",
      "1. **Quality**: Generally, customers appreciate the quality of the...\n",
      "\n",
      "Query: What do customers like most about this product?\n",
      "Analysis: Based on the customer review excerpts, the key features that customers appreciate about the product include:\n",
      "\n",
      "1. **Quality**: Multiple reviews highlight the good quality of the jeans, indicating that ...\n",
      "\n",
      "Query: What complaints or issues do customers mention?\n",
      "Analysis: Based on the provided customer review excerpts, the following complaints or issues are mentioned:\n",
      "\n",
      "1. **Sizing Issues**: One customer reported dissatisfaction with the size they ordered, specifically ...\n",
      "\n",
      "Query: What makes this product unique or special?\n",
      "Analysis: Based on the customer review excerpts, the product, likely a pair of jeans, has several unique or special features that stand out:\n",
      "\n",
      "1. **Trendy Design**: Customers consistently mention that the jeans ...\n",
      "\n",
      "âœ… Saved product features to review_analysis_output/baggy_jeans/product_features.json\n",
      "\n",
      "### Extracting Product Features for chicken_drumsticks ###\n",
      "\n",
      "Query: What are the main features and characteristics of this product?\n",
      "Analysis: Based on the customer review excerpts, the main features and characteristics of the product can be summarized as follows:\n",
      "\n",
      "1. **Quality and Freshness**: Many customers appreciate the quality of the ch...\n",
      "\n",
      "Query: What do customers like most about this product?\n",
      "Analysis: Customers appreciate several key features of the product based on the reviews:\n",
      "\n",
      "1. **Flavor and Taste**: Many customers highlight the good flavor of the chicken, with several mentioning that their ent...\n",
      "\n",
      "Query: What complaints or issues do customers mention?\n",
      "Analysis: Based on the customer review excerpts, the following complaints or issues are mentioned:\n",
      "\n",
      "1. **Product Quality Issues**: \n",
      "   - Some customers reported receiving damaged chicken that smelled bad and wa...\n",
      "\n",
      "Query: What makes this product unique or special?\n",
      "Analysis: Based on the customer review excerpts, the product stands out for several key features:\n",
      "\n",
      "1. **Flavor Quality**: Many customers highlight the exceptional flavor of the chicken, describing it as the \"be...\n",
      "\n",
      "âœ… Saved product features to review_analysis_output/chicken_drumsticks/product_features.json\n"
     ]
    }
   ],
   "source": [
    "def extract_product_features(product_name, index, sentences_df, embedding_model):\n",
    "    \"\"\"\n",
    "    Extract key product features using RAG.\n",
    "    \"\"\"\n",
    "    print(f\"\\n### Extracting Product Features for {product_name} ###\")\n",
    "    \n",
    "    queries = [\n",
    "        \"What are the main features and characteristics of this product?\",\n",
    "        \"What do customers like most about this product?\",\n",
    "        \"What complaints or issues do customers mention?\",\n",
    "        \"What makes this product unique or special?\"\n",
    "    ]\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        \n",
    "        # Retrieve relevant sentences\n",
    "        relevant = rag_retrieve(query, index, sentences_df, embedding_model, k=15)\n",
    "        \n",
    "        # Analyze with LLM\n",
    "        analysis = rag_llm_analysis(\n",
    "            query=query,\n",
    "            context_sentences=relevant,\n",
    "            task_description=\"identify and summarize key product features from customer reviews\"\n",
    "        )\n",
    "        \n",
    "        features[query] = {\n",
    "            'analysis': analysis,\n",
    "            'top_sentences': relevant.head(5)[['sentence', 'rating', 'relevance_score']].to_dict('records')\n",
    "        }\n",
    "        \n",
    "        print(f\"Analysis: {analysis[:200]}...\")\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = f'review_analysis_output/{product_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(f'{output_dir}/product_features.json', 'w') as f:\n",
    "        json.dump(features, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Saved product features to {output_dir}/product_features.json\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract for both products\n",
    "features_jeans = extract_product_features('baggy_jeans', index_jeans, sentences_jeans, model_jeans)\n",
    "features_chicken = extract_product_features('chicken_drumsticks', index_chicken, sentences_chicken, model_chicken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 8: Analysis Module 3 - Image Generation Prompts\n",
    "\n",
    "Generate detailed prompts for diffusion models based on review analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generating Image Prompts for baggy_jeans ###\n",
      "\n",
      "âœ… Generated 3 image prompts\n",
      "âœ… Saved to review_analysis_output/baggy_jeans/image_generation_prompts.json\n",
      "\n",
      "ðŸ“ Generated Prompts:\n",
      "\n",
      "1. Create an image of a pair of high-waisted baggy jeans in a dark'ish navy blue color, featuring trendy yellow stitching that pops against the denim. The jeans should showcase cute metal rivets on the pockets and have a relaxed fit, with big pockets visible on the sides. The overall aesthetic should reflect a hip-hop and cool-girl style, emphasizing comfort and casual fashion.\n",
      "\n",
      "2. Illustrate a stylish pair of baggy jeans made from medium-weight denim, appearing slightly worn for a casual look. Highlight the distinct yellow embroidery details along the seams and the cute metal rivets on the pockets, while ensuring the jeans have a flattering silhouette. The background should be minimalistic to enhance the focus on the jeans, conveying a trendy, youthful vibe.\n",
      "\n",
      "3. Design an image featuring a pair of baggy jeans in a relaxed fit, showcasing a combination of dark'ish navy blue fabric and striking yellow stitching. Include details such as large pockets and metal rivets that accentuate the jeans' stylish design. The scene should evoke a modern urban atmosphere, appealing to a fashionable audience seeking comfort and style.\n",
      "\n",
      "### Generating Image Prompts for chicken_drumsticks ###\n",
      "\n",
      "âœ… Generated 3 image prompts\n",
      "âœ… Saved to review_analysis_output/chicken_drumsticks/image_generation_prompts.json\n",
      "\n",
      "ðŸ“ Generated Prompts:\n",
      "\n",
      "1. Create an image of a pack of fresh chicken drumsticks displayed on a wooden kitchen counter. The drumsticks should appear plump and juicy, showcasing a rich brown color with slight sheen, while some pieces may have a hint of greyish tones that indicate potential quality concerns. Include a few feathers scattered nearby to reflect customer feedback on processing quality, and incorporate a rustic backdrop to enhance the home-cooked feel.\n",
      "\n",
      "2. Illustrate a family-friendly kitchen scene featuring a plate of cooked chicken drumsticks, beautifully glazed and golden brown, served alongside fresh vegetables. The chicken should look tender, juicy, and appetizing, with the textures of the meat clearly visible. Add a nice size packet of raw chicken in the background, indicating its appealing packaging, and ensure the overall warm lighting conveys a comforting and inviting atmosphere.\n",
      "\n",
      "3. Design an artistic composition that highlights a variety of fresh chicken products in an elegant grocery display. Showcase the chicken drumsticks and chopped meat with a focus on their textures, emphasizing the smooth surfaces and natural colors ranging from light browns to deeper shades. Incorporate elements of packaging that suggest convenience, such as neatly arranged family-size packets, and use a clean, modern aesthetic to reflect quality and freshness.\n"
     ]
    }
   ],
   "source": [
    "def generate_image_prompts(product_name, visual_info, features_info, product_category):\n",
    "    \"\"\"\n",
    "    Generate image generation prompts based on analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n### Generating Image Prompts for {product_name} ###\")\n",
    "    \n",
    "    # Combine visual and feature information\n",
    "    context = f\"\"\"\n",
    "Product Category: {product_category}\n",
    "\n",
    "Visual Information Summary:\n",
    "{json.dumps({k: v['analysis'] for k, v in visual_info.items()}, indent=2)}\n",
    "\n",
    "Feature Information Summary:\n",
    "{json.dumps({k: v['analysis'] for k, v in features_info.items()}, indent=2)}\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate prompts using LLM\n",
    "    prompt_request = \"\"\"\n",
    "Based on the customer review analysis above, generate 3 detailed image generation prompts for DALL-E or Stable Diffusion.\n",
    "\n",
    "Each prompt should:\n",
    "1. Capture the visual essence of the product from customer descriptions\n",
    "2. Include specific details about colors, textures, materials, and style\n",
    "3. Be optimized for text-to-image generation\n",
    "4. Be 2-3 sentences long\n",
    "\n",
    "Format your response as:\n",
    "PROMPT 1: [detailed prompt]\n",
    "PROMPT 2: [detailed prompt]\n",
    "PROMPT 3: [detailed prompt]\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at creating detailed prompts for AI image generation models.\"},\n",
    "        {\"role\": \"user\", \"content\": context + \"\\n\\n\" + prompt_request}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    generated_prompts = response.choices[0].message.content\n",
    "    \n",
    "    # Parse prompts\n",
    "    prompts_list = []\n",
    "    for line in generated_prompts.split('\\n'):\n",
    "        if line.startswith('PROMPT'):\n",
    "            prompt_text = line.split(':', 1)[1].strip() if ':' in line else line\n",
    "            prompts_list.append(prompt_text)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = f'review_analysis_output/{product_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    prompt_data = {\n",
    "        'product_name': product_name,\n",
    "        'category': product_category,\n",
    "        'generated_prompts': prompts_list,\n",
    "        'full_response': generated_prompts\n",
    "    }\n",
    "    \n",
    "    with open(f'{output_dir}/image_generation_prompts.json', 'w') as f:\n",
    "        json.dump(prompt_data, f, indent=2)\n",
    "    \n",
    "    # Also save as text file for easy reading\n",
    "    with open(f'{output_dir}/image_generation_prompts.txt', 'w') as f:\n",
    "        f.write(generated_prompts)\n",
    "    \n",
    "    print(f\"\\nâœ… Generated {len(prompts_list)} image prompts\")\n",
    "    print(f\"âœ… Saved to {output_dir}/image_generation_prompts.json\")\n",
    "    \n",
    "    print(\"\\nðŸ“ Generated Prompts:\")\n",
    "    for i, prompt in enumerate(prompts_list, 1):\n",
    "        print(f\"\\n{i}. {prompt}\")\n",
    "    \n",
    "    return prompt_data\n",
    "\n",
    "# Generate for both products\n",
    "prompts_jeans = generate_image_prompts(\n",
    "    'baggy_jeans', \n",
    "    visual_jeans, \n",
    "    features_jeans,\n",
    "    'Clothing - Baggy Jeans'\n",
    ")\n",
    "\n",
    "prompts_chicken = generate_image_prompts(\n",
    "    'chicken_drumsticks',\n",
    "    visual_chicken,\n",
    "    features_chicken,\n",
    "    'Food - Fresh Chicken'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2 STEP 9: Summary and Verification\n",
    "\n",
    "Verify all outputs and generate summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q3: Image Generation with Diffusion Models \n",
    "\n",
    "**To be implemented after Q2 is complete**\n",
    "\n",
    "This section will:\n",
    "- Use prompts from Q2 to generate images\n",
    "- Test with 2 different models (DALL-E, Stable Diffusion)\n",
    "- Compare generated images with actual product images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Image generation - to be implemented\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

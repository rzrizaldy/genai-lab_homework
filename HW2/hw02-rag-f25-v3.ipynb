{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62fb1bf8-b7a7-4833-95c0-9e5c4defb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json \n",
    "import os \n",
    "import io\n",
    "import re\n",
    "#import requests \n",
    "import dotenv \n",
    "import transformers\n",
    "import pypdf\n",
    "import faiss\n",
    "#import sqlite3\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import pipeline, BertTokenizer, BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "#from operator import itemgetter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17986735-76c2-48b1-8d31-bcf55415e9a2",
   "metadata": {},
   "source": [
    "# **Setting up Python Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46361200-b3d1-4823-9303-b0f8d03dad21",
   "metadata": {},
   "source": [
    "## Instructions for setting up your .env file:\n",
    "\n",
    "1. Create a .env file in the same directory as this notebook\n",
    "\n",
    "2. Add the following lines to the .env file:\n",
    "\n",
    "    OPENAI_API_KEY=<your_openai_api_key>\n",
    "\n",
    "    HF_TOKEN=<your_huggingface_token>\n",
    "\n",
    "3. Replace the placeholders with your actual keys.\n",
    "\n",
    "4. Save the file.\n",
    "\n",
    "5. Restart the kernel to ensure the keys are loaded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9284d476-6781-4e88-8116-c26483813fe3",
   "metadata": {},
   "source": [
    "# Load API Keys into the Notebook Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755afda5-3489-41c9-9ec9-238d5f80222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai = os.getenv('OPENAI_API_KEY')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb469919-5b78-4ecc-bff8-6b7a8cdb6a19",
   "metadata": {},
   "source": [
    "# Custom Functions for Chunking the CMU Student Handbook & Measuring Computational Cost\n",
    "(Optional / not required for the homework that you use these functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19fd865d-e28b-40e9-a799-eae3d7699209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Text into Sentences\n",
    "def split_text_into_sentences_v1(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def split_text_into_sentences_v2(text):\n",
    "    sentences = sent_tokenize(text, language='english')  # Default is usually 'english'\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b90dde5c-3e57-4064-9866-534125251e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to split the resumes into sentences and assign unique identifiers:\n",
    "\n",
    "def split_resumes_to_sentences(df, text_column):\n",
    "    \"\"\"\n",
    "    Split the resumes into individual sentences and assign unique identifiers.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the resumes.\n",
    "        text_column (str): The name of the column containing the resume texts.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with each sentence and its corresponding unique identifier.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to hold the resulting data\n",
    "    sentences_list = []\n",
    "    \n",
    "    # Iterate through the DataFrame rows\n",
    "    for idx, row in df.iterrows():\n",
    "        # Tokenize the resume text into sentences\n",
    "        sentences = sent_tokenize(row[text_column])\n",
    "        \n",
    "        # Append each sentence along with the original index to the list\n",
    "        for sentence in sentences:\n",
    "            sentences_list.append((idx, sentence))\n",
    "    \n",
    "    # Convert the list to a DataFrame\n",
    "    sentences_df = pd.DataFrame(sentences_list, columns=['unique_identifier', 'sentence'])\n",
    "    \n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a6edf2f-e975-4eb4-89ac-ad490332db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_embedding_costs(text, model_name='all-MiniLM-L6-v2', eps=0.6, min_samples=2):\n",
    "    \"\"\"\n",
    "    Computes the computational cost (in terms of execution time) for creating\n",
    "    sentence embeddings and paraphrase embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to be processed.\n",
    "    - model_name (str): The name of the model to use for embedding.\n",
    "    - eps (float): The epsilon value for DBSCAN clustering.\n",
    "    - min_samples (int): The minimum sample count for DBSCAN clustering.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the execution times for sentence embeddings and paraphrase-level embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Sentence Embedding Timing\n",
    "    start_time = time.time()\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    sentence_embedding_time = time.time() - start_time\n",
    "\n",
    "    # Paraphrase Embedding (Clustering) Timing\n",
    "    start_clustering_time = time.time()\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit(sentence_embeddings)\n",
    "    cluster_labels = clustering.labels_\n",
    "    \n",
    "    paraphrase_embeddings = []\n",
    "    for cluster_id in set(cluster_labels):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_sentences = np.array(sentences)[cluster_labels == cluster_id]\n",
    "        paraphrase = ' '.join(cluster_sentences)\n",
    "        paraphrase_embeddings.append(paraphrase)\n",
    "    paraphrase_embedding_time = time.time() - start_clustering_time\n",
    "\n",
    "    return sentence_embedding_time, paraphrase_embedding_time\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text = (\"This is a sample text. It has several sentences, meant to showcase \"\n",
    "            \"how embeddings are computed. Some of these sentences may be clustered \"\n",
    "            \"together, representing paraphrases or semantically similar groups.\")\n",
    "\n",
    "    sent_time, para_time = compute_embedding_costs(text)\n",
    "    print(f\"Sentence Embedding Time: {sent_time:.4f} seconds\")\n",
    "    print(f\"Paraphrase Embedding Time: {para_time:.4f} seconds\")\n",
    "\n",
    "# This function was created with GenerativeAI Assistance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d607795-c8dd-40c3-b704-282c7e4ed705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_flops(model_name, text):\n",
    "    \"\"\"\n",
    "    Estimate the FLOPs for generating embeddings for a given text using a specified model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Model identifier from Hugging Face Transformers.\n",
    "    - text (str): Text to process.\n",
    "\n",
    "    Returns:\n",
    "    - FLOPs (int): An estimated number of floating point operations.\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    # Hooks for the operations\n",
    "    def hook_fn_forward(module, input, output):\n",
    "        # Attempt to access the tensor shape in a safer manner\n",
    "        input_shape = input[0].size()\n",
    "        \n",
    "        # A generalized fallback if shape isn't what's expected\n",
    "        if len(input_shape) == 2:  # Assuming shape [batch, seq_len] for simplicity\n",
    "            batch_size, seq_len = input_shape\n",
    "            # Hypothetical FLOPs calculation: For demonstration, let's assume it's just the product\n",
    "            flops = batch_size * seq_len\n",
    "        elif len(input_shape) > 2:  # Assuming more dimensions (e.g., embeddings)\n",
    "            flops = torch.prod(torch.tensor(input_shape))\n",
    "        else:\n",
    "            # In case of unsupported dimensions, set flops to 0 or some placeholder\n",
    "            flops = 0\n",
    "\n",
    "        # Storing calculated FLOPs in the module\n",
    "        if hasattr(module, '__flops__'):\n",
    "            module.__flops__ += flops\n",
    "        else:\n",
    "            module.__flops__ = flops\n",
    "\n",
    "    def add_hooks_to_model(model, hook_fn):\n",
    "        \"\"\"\n",
    "        Recursively add hook_fn to all the layers of the model.\n",
    "        \"\"\"\n",
    "        total_flops = 0\n",
    "        for layer in model.children():\n",
    "            if list(layer.children()):  # if the layer has children, recursively add hooks\n",
    "                total_flops += add_hooks_to_model(layer, hook_fn)\n",
    "            else:\n",
    "                if hasattr(layer, 'weight'):\n",
    "                    layer.register_forward_hook(hook_fn)\n",
    "                    layer.__flops__ = 0\n",
    "        return total_flops\n",
    "\n",
    "    add_hooks_to_model(model, hook_fn_forward)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "\n",
    "    total_flops = sum([mod.__flops__ for mod in model.modules() if hasattr(mod, '__flops__')])\n",
    "\n",
    "    return total_flops\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    text = \"This is an example sentence\"\n",
    "    flops = estimate_model_flops(model_name, text)\n",
    "    print(f\"Estimated FLOPs: {flops}\")\n",
    "\n",
    "# This function was created with GenerativeAI Assistance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17ec0f-f60b-40e1-a7d0-d145aa07b3f7",
   "metadata": {},
   "source": [
    "# Load Data into the Notebook Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6af6cf-349f-496b-a9ad-9aa212f2530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(\"the-word-2023-24-12.11.23.pdf\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db76544-33a2-4628-a69e-25b853c38d73",
   "metadata": {},
   "source": [
    "# **Homework 2 Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83696b0-7564-4dc8-94f7-81281091796e",
   "metadata": {},
   "source": [
    "## **Section A. Experimenting with Vector Store Query Design (50 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887d28e-de42-4af8-9f7f-82e7e93ad668",
   "metadata": {},
   "source": [
    "### **Choose a method to chunk the text data:**\n",
    "\n",
    "- [Semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)\n",
    "\n",
    "- [Recursive chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
    "\n",
    "- [Character chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter)\n",
    "\n",
    "- [Token chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db0c94-cfb3-42c1-b0f7-4037db2cdf49",
   "metadata": {},
   "source": [
    "# Sentence Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83fd395-33d3-41f7-8d4d-ee8f7e62b254",
   "metadata": {},
   "source": [
    "### Choose a type of chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31c96c68-c704-42de-9544-695a56db8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example chunker. You don't have to use it. Email Sara with questions.\n",
    "\n",
    "# parser to split up PDF resume:\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f3566-a26d-4146-8e2d-8ea7726c8ce4",
   "metadata": {},
   "source": [
    "#### **Chunker Choices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d1cf5-ffe4-4ff8-9380-fd63ffcaa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunker choice #1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfcaa4-99ed-47b8-84e4-e525af7f818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunker choice #2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7afb62b6-a2ea-4447-b82d-d38ad43b366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code, feel free to use in homework. \n",
    "\n",
    "text_chunks = [] # create an empty list to store the text chunks.\n",
    "doc_idxs = []    # create an empty list to store unique identifiers for the text chunks.\n",
    "\n",
    "# split the CMU handbook up into chunks and assign unique identifiers to each chunk:\n",
    "for doc_idx, page in enumerate(doc):\n",
    "    page_text = page.get_text(\"text\")\n",
    "    cur_text_chunks = text_parser.split_text(page_text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7fe05-91a8-4a22-aca7-db6efb961634",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks # glance at the text chunks to observe how the chunks look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "928553a7-0bbf-427c-a2b0-4893bec03942",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk_df = pd.DataFrame(text_chunks) # put the chunks into a pandas dataframe.\n",
    "text_chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe5f1b8c-b4ac-4d2e-a0b6-c7cd25a4bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split resumes into sentences and include a unique identifier for each sentence:\n",
    "sentences_df = split_resumes_to_sentences(text_chunk_df, 0) \n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30660ffe-e711-498b-bc65-f3dd19d4d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the length of the sentences dataframe:\n",
    "len(sentences_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e69bd3-6452-4038-a25a-89ce525d9eac",
   "metadata": {},
   "source": [
    "### **Choose an embedding model to use for creating embeddings of the text chunks and create the Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94b79425-9800-41dc-88f3-4917b6d9bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create embeddings for the sentences:\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens') # here we are selecting to use a Bert model on HuggingFace to create the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26855dc8-ed8c-4881-9823-ba9f3b039035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence embeddings:\n",
    "sentence_embeddings = model.encode(sentences_df['sentence'])\n",
    "\n",
    "# check the shape of the sentence embeddings:\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e4c22-8f3f-4f9e-b104-4f974d147955",
   "metadata": {},
   "source": [
    "## **Create a FAISS Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "88997ec0-5616-492e-89ba-cc606e4d320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the dimensions of the sentence embeddings:\n",
    "dimension = sentence_embeddings.shape[1]\n",
    "\n",
    "# specify the number of sentences:\n",
    "nb = len(set(sentences_df))\n",
    "\n",
    "# specify the number of queries:\n",
    "nq = 10000 \n",
    "np.random.seed(1234)             # set a random number to make the process reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "\n",
    "#\n",
    "nlist = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cb00b265-1c79-422c-8012-a94f24899ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glance at the shape of the sentence embeddings or dimension for the vector store:\n",
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d237d6f1-0b96-44d3-8661-388d7a4b994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index for the vector store:\n",
    "index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e36853f3-1cbf-4012-8d59-295a67d24c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the sentence embeddings to the index:\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b4a826a7-f9f1-4c1e-8e35-d2ec70c280a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of vectors in the index:\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e8b55033-2d42-49a8-919d-48ed684f4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the index:\n",
    "index.train(sentence_embeddings)\n",
    "\n",
    "index.is_trained  # check if index is now trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6ca34-f789-45f3-9fb9-eb58974bb791",
   "metadata": {},
   "source": [
    "### **Construct Query and Perform Search of the Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "43e769d0-5577-480f-8d43-c1272c41ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a query to submit to the vector store:\n",
    "question = \"<INSERT QUERY FROM HOMEWORK ASSIGNMENT INSTRUCTIONS HERE>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "92d70a36-4b5e-47ad-b6d4-cc1a1cdca198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of documents to retrieve from the vector store in response to the query:\n",
    "retrival_number=10\n",
    "\n",
    "# create an embedding for the query:\n",
    "query_embedding = model.encode([question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5be560e5-25b0-44d7-9cc8-9c66f811bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    " # measure the time it takes to search the index\n",
    "D, I = index.search(query_embedding, retrival_number)  # search the index for the query, using the number of documents to retrieve specified by k\n",
    "print(I) # print the indices of the documents that are most similar to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff0fd592-49f8-4a42-b098-1fd509148c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and print the string data from 'text' column of the first index in I\n",
    "\n",
    "first_index = I[0] # Get the first index from I\n",
    "\n",
    "first_row_string = sentences_df['sentence'].iloc[first_index].sum()  # Use iloc to access the row by index\n",
    "\n",
    "print(first_row_string) # Print the string data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3a419-a504-4db6-a2e8-0ae9a040ea93",
   "metadata": {},
   "source": [
    "### **Define System Prompt (e.g. context message) to send to LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4672c947-23a1-455a-9994-daaf5ce8ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get retrieve the results from the vector store:\n",
    "def get_sys_message(user_query: str, retrieval_number: int):\n",
    "    query_embedding = model.encode([user_query])\n",
    "    D, I = index.search(query_embedding, retrival_number)  # search\n",
    "    first_index = I[0]  # Get the first index from I\n",
    "    first_row_string = sentences_df['sentence'].iloc[first_index].sum()\n",
    "    return first_row_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "942a72a9-9a8a-4b10-9426-9d9bf7669aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the custom function to retrieve the results from the vector store:\n",
    "get_sys_message(user_query=\"Which resume has the most software skills listed?\", retrieval_number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b6076f29-0483-4d97-8069-d23ac1fec67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function for using an LLM with a RAG retriever:\n",
    "def rag_openAI_gpt(\n",
    "    model: str, \n",
    "    query: str, \n",
    "    retrieval_number: int, \n",
    "    llm_prompt: str):\n",
    "    \n",
    "    import openai\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    f=get_sys_message(query, retrieval_number)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"Instruction: use the information in {f} to answer the user's question.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{llm_prompt}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{f}\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the answer?\"}\n",
    "    ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "139db81c-c3ff-4c43-b44b-43f5b47aacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_3_5_turbo = \"gpt-3.5-turbo\"\n",
    "gpt_4 = \"gpt-4\"\n",
    "gpt_4_turbo = \"gpt-4-0125-preview\"\n",
    "gpt_4o = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416965f-d61f-49c8-8db0-dc2eb743e13a",
   "metadata": {},
   "source": [
    "## Examples for demonstration only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cb937223-a9a9-45d8-92e9-d9f5540c7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_openAI_gpt(model=gpt_3_5_turbo, query=\"Which resume has the most software skills listed?\", retrieval_number=20, llm_prompt=\"Classify the document and return a label based on the document type or class. Make the label specify which occupation the document pertains to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "201c0a76-ff82-4152-938e-b8c8023506ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_openAI_gpt(model=gpt_4, query=\"Which resume has the most software skills listed?\", retrieval_number=20, llm_prompt=\"summarize the resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1acb53b0-2425-4731-9360-c82ed6ca6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_openAI_gpt(model=gpt_4_turbo, query=\"Which resume has the most software skills listed?\", retrieval_number=20, llm_prompt=\"summarize the resume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44624b99-11f4-4047-b852-769b5ed0f2b5",
   "metadata": {},
   "source": [
    "# Homework requirement:\n",
    "\n",
    "# Section A\n",
    "\n",
    "## **Query the vector store using these queries**\n",
    "\n",
    "**Instruction: set the 'k' parameter to 5**\n",
    "\n",
    "Query 1: What is the policy statement for the academic integrity policy?\n",
    "\n",
    "Query 2: What is the policy violation definition for cheating?\n",
    "\n",
    "Query 3: What is the policy statement for improper or illegal communications?\n",
    "\n",
    "Query 4: What are CMU\u2019s quiet hours?\n",
    "\n",
    "Query 5: Where are pets allowed on CMU?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b493d-8963-4bad-b28d-fcd5a21a2b19",
   "metadata": {},
   "source": [
    "### ***query the vector store with the 5 queries above (don't forget to record the responses in your homework submission spreadsheet: see instructions for a link to the spreadsheet!):***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2c51d113-3e36-42d8-8bd1-a97e90a0f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the vector store with the 5 queries above (don't forget to record the responses in your homework submission!):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8b400-0f50-4ab0-87d3-994923686751",
   "metadata": {},
   "source": [
    "# **Homework Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a7702-a7f1-42f0-b17d-4289df5e29e7",
   "metadata": {},
   "source": [
    "**A.I.** \n",
    "\n",
    "(i) Describe these distance metrics: Cosine similarity; Euclidean Distance; Dot Product.\n",
    "\n",
    "(ii) For each of the metrics you defined in (i), describe how the metric is different from the other metrics.\n",
    "\n",
    "(iii) For each of the metrics you defined in (i), describe one advantage and one disadvantage of using the metric.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb24a3-d038-46cb-80a3-4ad10a1a115d",
   "metadata": {},
   "source": [
    "**A.II.** Copy and paste the results or information retrieved from the vector store in response to each of the queries you submitted to the vector store in the SPREADSHEET TEMPLATE (please see instructions for a link to the spreadsheet template you should copy and use).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ccf17-c3c1-4b6a-a255-b128453c388a",
   "metadata": {},
   "source": [
    "**A.III.** Qualitatively analyze the responses to your queries submitted to the vector store. Did the queries retrieve the information you were expecting to obtain. Why or why not? Why do you think the queries were successful / unsuccessful in retrieving the information you expected or needed? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4c0dc-1866-4edf-89d8-79fbb987ecdd",
   "metadata": {},
   "source": [
    "# **Section B. Experimenting with Vector Store Embeddings & Query Parameters (50 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d0980-45c3-4745-a7ff-fa5100fdc98d",
   "metadata": {},
   "source": [
    "1) Choose 1 of the 5 queries provided in Section A, above, and experiment with submitting the query to the vector store by changing the QUERY and RETRIEVAL_NUMBER parameters in the following manner:\n",
    "\n",
    "\n",
    "*   A) Baseline query (e.g. query), retrieval_number parameter=1.\n",
    "\n",
    "*   B) Query, retrieval_number parameter  = 3\n",
    "\n",
    "*   C) Query, retrieval_number parameter  = 5\n",
    "\n",
    "*   D) Query, retrieval_number parameter  = 10\n",
    "\n",
    "**In your written homework submission, record the UNIQUE responses/results of each query submitted to the vector store.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff92b1-6630-4735-8878-9e9b134901ed",
   "metadata": {},
   "source": [
    "2. Select a different text chunking method (e.g. word, sentence, paragraph) and:\n",
    "   \n",
    "- Chunk your text data using the method.\n",
    "- Create embeddings for the text. \n",
    "- Load the embeddings into the vector store. \n",
    "- Submit the same query you selected in B.1, above, and submit it to the vector store 6 times (using the different \u2018retrieval_number\u2019 parameter settings defined in B.1, above), and record the responses.\n",
    "\n",
    "**In your written homework submission, record the responses/results of each query submitted to the vector store.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a7f38-d0b5-4cfa-be66-e040aecbfd16",
   "metadata": {},
   "source": [
    "### **Homework Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308cc5e-9427-487f-b767-b718da043126",
   "metadata": {},
   "source": [
    "**B.I.** Explain your rationale for selecting the query you choose in B.1. Why did you choose this query vs. the other 4 queries? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba4b54-41a0-4a94-a38d-18aecc880459",
   "metadata": {},
   "source": [
    "**B.II.** Copy and paste the responses to the queries you submitted to the vector store in the SPREADSHEET TEMPLATE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa231e83-940a-4afb-98ec-68bc85c2cf45",
   "metadata": {},
   "source": [
    "**B.III.** Copy and paste the responses to the queries you submitted to the vector store in the SPREADSHEET TEMPLATE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b5f32-00d7-46b7-97a7-f043974954c0",
   "metadata": {},
   "source": [
    "**B.IV.** In observing the responses from the vector store to the queries created in B.1., which \u2018k\u2019 parameter do you think retrieved the highest quality / most accurate result? Why do you think this parameter was the best to use with the query?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3347d66-9646-49fc-b93a-5839a9d52c92",
   "metadata": {},
   "source": [
    "**B.V.** In observing the responses from the vector store to the queries created in B.2., which \u2018k\u2019 parameter do you think retrieved the highest quality / most accurate result? Why do you think this parameter was the best to use with the query?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa20517-9e92-4ac3-aa98-86308033c6c0",
   "metadata": {},
   "source": [
    "# **BONUS TASKS / QUESTIONS: Define function to call LLM API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f31ada-df59-439e-8a11-7ffb05848002",
   "metadata": {},
   "source": [
    "## Please email Sara for the Bonus Task Python Notebook once you've completed your homework assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d904d1-c996-4f65-8457-cc93f771cd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
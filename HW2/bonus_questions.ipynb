{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rzrizaldy/CodeFolder/genai-lab_homework/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rzrizaldy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading : Package '' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub, datasets, transformers, os, sys, json, random, re, nltk, time, requests, faiss\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rzrizaldy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import llm_functions, rag_functions, evaluation_metrics, data_functions\n",
    "# FYI: It's necessary to have these .PY files in the same directory as this Python Notebook and an __init__.py file needs to be in the same directory to import the functions from these Python files. \n",
    "# Email Sara Kingsley or the Teaching Assistants with any issues importing these.\n",
    "\n",
    "from llm_functions import LLM_requesters\n",
    "from rag_functions import ragLLM\n",
    "from  evaluation_metrics import RagMetrics\n",
    "from data_functions import DataFunctions\n",
    "\n",
    "llm = LLM_requesters()\n",
    "ragllm = ragLLM()\n",
    "metrics = RagMetrics()\n",
    "datafunc = DataFunctions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the required API keys from your .env file:\n",
    "load_dotenv()\n",
    "\n",
    "openai = os.getenv('OPENAI_API_KEY')       # OpenAI API Key. \n",
    "hf_token = os.getenv('HF_TOKEN')    # HuggingFaceHub Authentication Token. \n",
    "HF_INF_TOKEN = os.getenv(\"HF_INF_TOKEN\")   # HuggingFace Inference Endpoint Token. \n",
    "\n",
    "gemma_endpoint = os.getenv(\"gemma_endpoint_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_01 = \"gpt-3.5-turbo\"\n",
    "model_02 = \"gpt-4o\"\n",
    "model_03=\"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The HuggingFace Inference endpoint URL (gemma_endpoint_url) was not found in your .env file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# There is an error because 'gemma_endpoint' is None (not set in the .env file),\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# so the HF endpoint URL is missing, and HF can't send the request. \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Even though HuggingFace credentials are loaded, the model endpoint URL must be given.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# To avoid error, make sure gemma_endpoint is not None:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gemma_endpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe HuggingFace Inference endpoint URL (gemma_endpoint_url) was not found in your .env file.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m input_message = {\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCan you please let us know more details about your \u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mparameters\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m100\u001b[39m\n\u001b[32m     13\u001b[39m     }\n\u001b[32m     14\u001b[39m }\n\u001b[32m     16\u001b[39m output = llm.query_llm_model(input_message, HF_INF_TOKEN, model_endpoint=gemma_endpoint)\n",
      "\u001b[31mValueError\u001b[39m: The HuggingFace Inference endpoint URL (gemma_endpoint_url) was not found in your .env file."
     ]
    }
   ],
   "source": [
    "input_message = {\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "\t\"parameters\": {\n",
    "\t\t\"max_new_tokens\": 100}\n",
    "}\n",
    "\n",
    "output = llm.query_llm_model(input_message, HF_INF_TOKEN, model_endpoint=gemma_endpoint) \n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Tasks Required to Answer Bonus Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Resume dataset into the Python environment:\n",
    "\n",
    "df_path = \"Resume.csv\"   # Note you will need to change this path to the location of the Resume.csv file on your local machine\n",
    "\n",
    "df=pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the column name to 'text'; this will format the dataset in the fashion required by some of the functions in this notebook\n",
    "df.rename(columns = {'Resume_str':'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200/754707001.py:10: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample_resume_df = df.groupby('Category').apply(lambda x: select_random_rows(x['text'], 10)).reset_index(drop=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>level_1</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>DIGITAL-MEDIA</td>\n",
       "      <td>1257</td>\n",
       "      <td>SENIOR DIRECTOR, PRODUCT MANAGEMENT  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>DIGITAL-MEDIA</td>\n",
       "      <td>1254</td>\n",
       "      <td>DIRECTOR OF NEW BUSINESS DEVELOPMENT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>DIGITAL-MEDIA</td>\n",
       "      <td>1252</td>\n",
       "      <td>SENIOR MARKETING MANAGER       Execut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>DIGITAL-MEDIA</td>\n",
       "      <td>1312</td>\n",
       "      <td>DIGITAL MARKETING MANAGER       Summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>DIGITAL-MEDIA</td>\n",
       "      <td>1311</td>\n",
       "      <td>DIGITAL MARKETING DIRECTOR           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>TEACHER</td>\n",
       "      <td>419</td>\n",
       "      <td>Marilyn    Hunter       Summary     Focus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>TEACHER</td>\n",
       "      <td>410</td>\n",
       "      <td>LEAD TEACHER       Summary     Solid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>TEACHER</td>\n",
       "      <td>416</td>\n",
       "      <td>Kimberly  Fisheli       Summary     Ded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>TEACHER</td>\n",
       "      <td>394</td>\n",
       "      <td>SUBSTITUTE TEACHER         Skills    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>TEACHER</td>\n",
       "      <td>393</td>\n",
       "      <td>SUBSTITUTE TEACHER           Skills  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Category  level_1                                               text\n",
       "140  DIGITAL-MEDIA     1257           SENIOR DIRECTOR, PRODUCT MANAGEMENT  ...\n",
       "141  DIGITAL-MEDIA     1254           DIRECTOR OF NEW BUSINESS DEVELOPMENT ...\n",
       "142  DIGITAL-MEDIA     1252           SENIOR MARKETING MANAGER       Execut...\n",
       "143  DIGITAL-MEDIA     1312           DIGITAL MARKETING MANAGER       Summa...\n",
       "144  DIGITAL-MEDIA     1311           DIGITAL MARKETING DIRECTOR           ...\n",
       "..             ...      ...                                                ...\n",
       "235        TEACHER      419       Marilyn    Hunter       Summary     Focus...\n",
       "236        TEACHER      410           LEAD TEACHER       Summary     Solid ...\n",
       "237        TEACHER      416         Kimberly  Fisheli       Summary     Ded...\n",
       "238        TEACHER      394           SUBSTITUTE TEACHER         Skills    ...\n",
       "239        TEACHER      393           SUBSTITUTE TEACHER           Skills  ...\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Create a sample dataset from the Resume dataset: \n",
    "Since the Resume dataset contains many rows, we will create a sample dataset with a smaller number of rows to work with.\n",
    "'''\n",
    "\n",
    "# Function to select 10 random rows from each group, where group is defined by the column named 'Category', and the row selection is from the column named 'text\n",
    "def select_random_rows(group, n=10):\n",
    "    return group.sample(n=min(n, len(group)))\n",
    "\n",
    "# Applying the function to each group in the column named 'Category':\n",
    "sample_resume_df = df.groupby('Category').apply(lambda x: select_random_rows(x['text'], 10)).reset_index(drop=False)\n",
    "\n",
    "sample_resume_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split resumes into sentences\n",
    "sentences_df = datafunc.split_resumes_to_sentences(sample_resume_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to store the chunks of text that we will create next:\n",
    "resume_sentences = []\n",
    "\n",
    "# split the text in each row of the 'text' column into sentences and store the sentences in the list:\n",
    "for row in sample_resume_df['text']:\n",
    "    sentences = datafunc.split_text_into_sentences(row)\n",
    "    resume_sentences.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42aebe0eb9c94403a78dec0bcb6238ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bc26a97f3644048f27f458426f1106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719ae9acd9184a939116cc53e587b76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9ad802cc12469b9c9e81e5c822ebbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187d04abf77643ae82aff7a8a1b0fcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc7a6e54cb94f0bb9e9b2e3fd3ef092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4baeea71f94039ac46f85406a3cc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4484608689d4c3bb05233d764412190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4f3d0fa04441a588c18ae7fe8d3cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cec250b9237430b88d61f420985573f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb23c8598c57408791167ca58ef7d9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d16b2ee10a54aeeb9f190d624535077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9151, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the model for creating embeddings:\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# create embeddings for the sentences:\n",
    "sentence_embeddings = model.encode(sentences_df['sentence'])\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FAISS Index, Add the sentence embeddings to the index and create a Vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d = sentence_embeddings.shape[1]\n",
    "\n",
    "nb = len(set(sentences))\n",
    "\n",
    "nq = 10000 \n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "nlist = 100\n",
    "\n",
    "import faiss\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index\n",
    "\n",
    "index.add(sentence_embeddings)\n",
    "\n",
    "index.ntotal\n",
    "\n",
    "index.train(sentence_embeddings)\n",
    "\n",
    "index.is_trained  # check if index is now trained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models we will use in the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_01 = \"gpt-3.5-turbo\"\n",
    "model_02 = \"gpt-4o\"\n",
    "model_03=\"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8429529 ,  0.64041024,  0.6867857 , ...,  0.4879449 ,\n",
       "         0.10311098,  0.3043144 ],\n",
       "       [-0.53987557,  0.7626106 ,  1.1137615 , ..., -0.32825473,\n",
       "         0.23717196,  0.6216686 ],\n",
       "       [ 0.12165318, -0.31398743,  1.1303501 , ..., -0.12323594,\n",
       "        -0.6754674 ,  0.07177585],\n",
       "       ...,\n",
       "       [ 0.20645209,  0.708416  ,  0.600371  , ...,  0.3412706 ,\n",
       "        -0.46779758, -0.32732195],\n",
       "       [ 0.3091744 ,  0.32063502,  1.7302022 , ..., -0.97805065,\n",
       "        -1.4428306 , -0.15120117],\n",
       "       [-0.453167  ,  1.0767179 ,  0.82169944, ..., -0.402537  ,\n",
       "        -0.20317224, -0.11159677]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Queries for the Experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "d = sentence_embeddings.shape[1]\n",
    "nb = len(set(sentences))\n",
    "nq = 10000 \n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "nlist = 100\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index\n",
    "index.add(sentence_embeddings)\n",
    "index.train(sentence_embeddings)\n",
    "print(index.is_trained)\n",
    "\n",
    "def get_sys_message(user_prompt, k):\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    k=k\n",
    "    xq = model.encode([user_prompt])\n",
    "    D, I = index.search(xq, k)  # search\n",
    "    first_index = I[0]  # Get the first index from I\n",
    "    rag_search_results = sentences_df['sentence'].iloc[first_index].sum()\n",
    "    return rag_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 3 system prompts to use in a call to different LLMs by querying the vector store with the queries given to you in the Bonus Task Instructions:\n",
    "(the questions below are examples; don't use these to complete the bonus tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expert computing and technology skills including competence in multiple software applications.Good understanding of Software Development Life Cycles and its different phases.Performed complexity installation and maintenance of software.Incluya conocimientos de paquetes de software y sistemas informáticos pertinentes e indique su nivel de conocimiento básico, intermedio, experto).Software management for Dept.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_messageA = get_sys_message(user_prompt=\"Which resume has the most software skills listed?\", k=5)\n",
    "sys_messageA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leadership and business accounting skills.Established process for professional services project accounts receivable, collections, and accounts payable bookkeeping systems which created a monthly cash flow that was not previously available.Developed expertise navigating financial management pipelines like accounting, reporting, internal controls, budgeting, analysis and performance management.Bookkeeping of small and midsized companies Worked on bank reconciliation, cash management and financial statements analysis.Financial Analyst  ,     07/2013   to   07/2014     Company Name          Selected to examine accounting records to compile financial information and reconcile reports.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_messageB = get_sys_message(user_prompt=\"Which resume has the most accounting skills listed?\", k=5)\n",
    "sys_messageB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Designed and built a novel spotting-device for producing protein microarrays compatible with a 96-well microplate format for high throughput applications.Experience      Company Name    City  ,   State    GRADUATE RESEARCH ASSISTANT   01/2013   to   04/2016       Assist with various research project in the Center of Biological control lab.Perform specialized tests such as aflatoxin and protein analysis; calibrates equipment, grind samples, read results and enters reading on official certificates.Total genomic DNA isolation & PCR & Cloning of polyene CYP gene \\n     (cytochrome P-450 hydroxlase).Programmed Labview spotting routine for the protein microarray spotting-device.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_messageC = get_sys_message(user_prompt=\"Which resume has the most biology skills listed?\", k=5)\n",
    "sys_messageC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request responses from 4 different LLMs, following the Bonus Task Instructions:\n",
    "(the questions below are examples; don't use these to complete the bonus tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textA=ragllm.rag_llm_openai(model=model_01, user_prompt=\"Which resume has the most software skills listed?\", k=10, task=\"summarize\", system_message=sys_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The candidate possesses expert computing and technology skills, competence in multiple software applications, a good understanding of Software Development Life Cycles, and has performed complex installations and maintenance of software. They have knowledge of relevant software packages and computer systems at an intermediate to expert level and have experience in software management for departmental tasks.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textB= ragllm.rag_llm_openai(model=model_02, user_prompt=\"Which resume has the most software skills listed?\", k=10, task=\"summarize\", system_message=sys_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The resume highlights strong computing and technological expertise, including proficiency in multiple software applications. It demonstrates a good understanding of Software Development Life Cycles and its various phases. The candidate has experience in the complex installation and maintenance of software and possesses knowledge of relevant software packages and computer systems, specifying their proficiency levels as basic, intermediate, or expert. Additionally, they manage software for their department to accomplish tasks.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e465e8e376642398aee50aba9797f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0429ecb3e84841fb9998be4ac6cb104d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88cfa35d51b4329b40bb5d466fe902c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3972185c540e4a46a522b6b4eefaab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb1a8244a504d968a038e63375ef702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb88bb8bcfb479dac65105eaef4a2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "textC=ragllm.rag_llm(model=model_03, user_prompt=\"Which resume has the most software skills listed?\", k=10, task=\"summarize\", system_message=sys_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "textD = \"<insert call to LLM of your chocie>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Information Retrieved and LLM responses by comparing the similarity of the responses across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text_similarity(text_a, text_b, text_c):\n",
    "    \"\"\"\n",
    "    Compares the similarity between three texts using TF-IDF vectors and cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - text_a (str): Text A\n",
    "    - text_b (str): Text B\n",
    "    - text_c (str): Text C\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with similarity scores between Text A & Text B, A & C, and B & C.\n",
    "    \"\"\"\n",
    "    # Initialize the vectorizer and transform texts into TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_a, text_b, text_c])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Similarity between Text A and B, A and C, and then B and C\n",
    "    similarity_scores = {\n",
    "        \"A_B\": similarity_matrix[0, 1],\n",
    "        \"A_C\": similarity_matrix[0, 2],\n",
    "        \"B_C\": similarity_matrix[1, 2]\n",
    "    }\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "def compare_text_similarity_response2context(om, model_response):\n",
    "    \"\"\"\n",
    "    Compares the similarity between three texts using TF-IDF vectors and cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - text_a (str): Text A\n",
    "    - text_b (str): Text B\n",
    "    - text_c (str): Text C\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with similarity scores between Text A & Text B, A & C, and B & C.\n",
    "    \"\"\"\n",
    "    # Initialize the vectorizer and transform texts into TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([om, model_response])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Similarity between Text A and B, A and C, and then B and C\n",
    "    similarity_scores = {\n",
    "        \"A_B\": similarity_matrix[0, 1],\n",
    "        #\"A_C\": similarity_matrix[0, 2],\n",
    "        #\"B_C\": similarity_matrix[1, 2]\n",
    "    }\n",
    "\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Similarity of LLM Response Messages A, B, and C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response Similarity Scores:\n",
      "A_B: 0.7036\n",
      "A_C: 0.0000\n",
      "B_C: 0.0000\n"
     ]
    }
   ],
   "source": [
    "similarities = compare_text_similarity(text_a=textA, text_b=textB, text_c=textC)\n",
    "\n",
    "print(\"LLM Response Similarity Scores:\")\n",
    "for pair, score in similarities.items():\n",
    "    print(f\"{pair}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Similarity of System Messages A, B, and C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Similarity -to-  System Prompt Similarity Scores:\n",
      "A_B: 0.1049\n",
      "A_C: 0.0769\n",
      "B_C: 0.1255\n"
     ]
    }
   ],
   "source": [
    "similaritiesOM =compare_text_similarity(text_a=sys_messageA, text_b=sys_messageB, text_c=sys_messageC)\n",
    "\n",
    "print(\"System Prompt Similarity -to-  System Prompt Similarity Scores:\")\n",
    "for pair, score in similaritiesOM.items():\n",
    "    print(f\"{pair}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Similarity of System Messages A, B, and C to LLM Response Messages A, B and C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt to LLM Response \n",
      " Similarity Scores \n",
      " Model 1:\n",
      "A_B: 0.5392\n",
      "\n",
      "\n",
      "System Prompt to LLM Response\n",
      " Similarity Scores \n",
      " Model 2:\n",
      "A_B: 0.1686\n",
      "\n",
      "\n",
      "System Prompt to LLM Response\n",
      " Similarity Scores \n",
      " Model 3:\n",
      "A_B: 0.0000\n"
     ]
    }
   ],
   "source": [
    "similaritiesOM_LLM_A =compare_text_similarity_response2context(om=sys_messageA, model_response=textA)\n",
    "similaritiesOM_LLM_B =compare_text_similarity_response2context(om=sys_messageB, model_response=textB)\n",
    "similaritiesOM_LLM_C =compare_text_similarity_response2context(om=sys_messageC, model_response=textC)\n",
    "\n",
    "print(\"System Prompt to LLM Response \\n Similarity Scores \\n Model 1:\")\n",
    "for pair, score in similaritiesOM_LLM_A.items():\n",
    "    print(f\"{pair}: {score:.4f}\")\n",
    "print(\"\\n\")\n",
    "print(\"System Prompt to LLM Response\\n Similarity Scores \\n Model 2:\")\n",
    "for pair, score in similaritiesOM_LLM_B.items():\n",
    "    print(f\"{pair}: {score:.4f}\")\n",
    "print(\"\\n\")\n",
    "print(\"System Prompt to LLM Response\\n Similarity Scores \\n Model 3:\")\n",
    "for pair, score in similaritiesOM_LLM_C.items():\n",
    "    print(f\"{pair}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
